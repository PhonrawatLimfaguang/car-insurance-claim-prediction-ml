# -*- coding: utf-8 -*-
"""Car Insurance Claim Prediction 175,249,584 Version.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hmxP0DNmxVU_SsPkjSpMXvWiv8ZzIwDJ

# Import Data
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("ifteshanajnin/carinsuranceclaimprediction-classification")

print("Path to dataset files:", path)

import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt

df_train = pd.read_csv(path + "/train.csv")

df_test = pd.read_csv(path + "/test.csv")

"""# EDA"""

pd.set_option('display.max_columns', None)

df_train.head()

df_train["model"].value_counts()

df_test.head()

df_train.columns

df_test.columns

df_train.shape

df_test.shape

df_train.info()

df_test.info()

df_train.isnull().sum()

df_test.isnull().sum()

df_train.duplicated().sum()

df_test.duplicated().sum()

df_train.nunique()

df_test.nunique()

train_uniques = df_train.nunique()
test_uniques = df_test.nunique()

compare = pd.DataFrame({
    "train_unique": train_uniques,
    "test_unique": test_uniques
})
print(compare)

df_train.describe().T

df_test.describe().T

df_train['is_claim'].value_counts(normalize=True)

for df, name in [(df_train,"Train"),(df_test,"Test")]:
    print(f"\nTop categories in {name}:")
    for col in df.select_dtypes(include='object').columns:
        print(f"\n{col}:\n", df[col].value_counts(normalize=True).head())

"""drop policy_id / show all unique IDs should drop this column before training"""

train_clean = df_train.copy()
test_clean = df_test.copy()

train_clean = train_clean.drop(columns=['policy_id'])
test_clean = test_clean.drop(columns=['policy_id'])

"""# EDA (Descriptive Statistics)"""

stats = pd.DataFrame()
numeric_cols = df_train.select_dtypes(include='number').columns

stats['Mean'] = df_train[numeric_cols].mean()
stats['Median'] = df_train[numeric_cols].median()
stats['Mode'] = df_train[numeric_cols].mode().iloc[0]
stats['Std Dev'] = df_train[numeric_cols].std()
stats['Variance'] = df_train[numeric_cols].var()
stats['Min'] = df_train[numeric_cols].min()
stats['Max'] = df_train[numeric_cols].max()
stats['Range'] = stats['Max'] - stats['Min']
stats['Q1'] = df_train[numeric_cols].quantile(0.25)
stats['Q3'] = df_train[numeric_cols].quantile(0.75)
stats['IQR'] = stats['Q3'] - stats['Q1']
stats['Skewness'] = df_train[numeric_cols].skew()
stats['Kurtosis'] = df_train[numeric_cols].kurtosis()
stats['Lower Bound'] = stats['Q1'] - 1.5 * stats['IQR']
stats['Upper Bound'] = stats['Q3'] + 1.5 * stats['IQR']

outlier_counts = {}
for col in numeric_cols:
    lower = stats.loc[col, 'Lower Bound']
    upper = stats.loc[col, 'Upper Bound']
    outliers = df_train[(df_train[col] < lower) | (df_train[col] > upper)][col]
    outlier_counts[col] = len(outliers)

stats['Outlier Count'] = pd.Series(outlier_counts)

stats

"""# EDA on Train (with target)"""

if "is_claim" in train_clean.columns:
    print("\nTrain - is_claim distribution:")
    print(train_clean['is_claim'].value_counts(normalize=True))

    sns.countplot(x="is_claim", data=train_clean)
    plt.title("Train: Claim Distribution")
    plt.show()

num_cols = train_clean.select_dtypes(include=['int64','float64']).columns.drop("is_claim")

for col in num_cols:
    plt.figure(figsize=(6,4))
    sns.histplot(df_train[col], kde=True, bins=30)
    plt.title(f"Distribution of {col} (Train)")
    plt.show()

cat_cols = train_clean.select_dtypes(include='object').columns

for col in cat_cols:
    plt.figure(figsize=(8,4))
    claim_rate = train_clean.groupby(col)['is_claim'].mean().sort_values()
    sns.barplot(x=claim_rate.index, y=claim_rate.values, palette="viridis")
    plt.title(f"Claim Rate by {col}")
    plt.xticks(rotation=45)
    plt.show()

plt.figure(figsize=(10,6))
corr = train_clean.select_dtypes(include='number').corr()
sns.heatmap(corr, annot=False, cmap="coolwarm", center=0)
plt.title("Correlation Heatmap (Train)")
plt.show()

"""# EDA on Test (sanity check)"""

for col in num_cols:
    plt.figure(figsize=(8,4))
    sns.kdeplot(train_clean[col], label="Train", shade=True)
    sns.kdeplot(test_clean[col], label="Test", shade=True)
    plt.title(f"Train vs Test Distribution: {col}")
    plt.legend()
    plt.show()

compare_uniques = pd.DataFrame({
    "train_unique": train_clean.nunique(),
    "test_unique": test_clean.nunique()
})
print("\nUnique Value Comparison (Train vs Test):")
print(compare_uniques)

for col in train_clean.select_dtypes(include='object').columns:
    train_cats = set(df_train[col].unique())
    test_cats = set(df_test[col].unique())

    only_in_train = train_cats - test_cats
    only_in_test = test_cats - train_cats

    if only_in_train:
        print(f"{col}: categories only in Train: {only_in_train}")
    if only_in_test:
        print(f"{col}: categories only in Test: {only_in_test}")

train_clean["is_claim"].value_counts()

train_clean.shape

"""# Data Preprocessing"""

train_clean.head()

train_clean.shape

stats.head(20)

train_clean.dtypes

# ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Boolean-like ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á
bool_columns = [

    'is_esc', 'is_adjustable_steering', 'is_tpms', 'is_parking_sensors',
    'is_parking_camera', 'is_front_fog_lights', 'is_rear_window_wiper',
    'is_rear_window_washer', 'is_rear_window_defogger', 'is_brake_assist',
    'is_power_door_locks', 'is_central_locking', 'is_power_steering',
    'is_driver_seat_height_adjustable', 'is_day_night_rear_view_mirror',
    'is_ecw', 'is_speed_alert'

]

# ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ 'Yes' ‡πÄ‡∏õ‡πá‡∏ô 1 ‡πÅ‡∏•‡∏∞ 'No' ‡πÄ‡∏õ‡πá‡∏ô 0 ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
for col in bool_columns:
    train_clean[col] = train_clean[col].map({'Yes': 1, 'No': 0})

# ‡πÅ‡∏¢‡∏Å‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'max_torque'
# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '60Nm@3500rpm'
train_clean[['Torque', 'Torque_unit_rpm']] = train_clean['max_torque'].str.split('@', expand=True)
train_clean['Torque_unit'] = train_clean['Torque'].str.extract('([a-zA-Z]+)')
train_clean['Torque'] = train_clean['Torque'].str.extract('([0-9.]+)')
train_clean['Torque_rpm'] = train_clean['Torque_unit_rpm'].str.extract('([0-9.]+)')

# ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
train_clean['Torque'] = pd.to_numeric(train_clean['Torque'])
train_clean['Torque_rpm'] = pd.to_numeric(train_clean['Torque_rpm'])

# ‡πÅ‡∏¢‡∏Å‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'max_power'
# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '40.36bhp@6000rpm'
train_clean[['Power', 'Power_unit_rpm']] = train_clean['max_power'].str.split('@', expand=True)
train_clean['Power_unit'] = train_clean['Power'].str.extract('([a-zA-Z]+)')
train_clean['Power'] = train_clean['Power'].str.extract('([0-9.]+)')
train_clean['Power_rpm'] = train_clean['Power_unit_rpm'].str.extract('([0-9.]+)')

# ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
train_clean['Power'] = pd.to_numeric(train_clean['Power'])
train_clean['Power_rpm'] = pd.to_numeric(train_clean['Power_rpm'])

# ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≠‡∏Å
train_clean = train_clean.drop(columns=['max_torque', 'max_power', 'Torque_unit_rpm', 'Power_unit_rpm'])

# ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Categorical ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏≥ One-Hot Encoding
categorical_columns = [
    'area_cluster', 'segment', 'model', 'fuel_type',
    'engine_type', 'rear_brakes_type', 'transmission_type',
    'steering_type','Torque_unit','Power_unit', 'gear_box'
]

# ‡∏ó‡∏≥ One-Hot Encoding
train_clean = pd.get_dummies(train_clean, columns=categorical_columns)

train_clean.head()

# ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ True/False ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 1/0
for col in train_clean.columns:
    if train_clean[col].dtype == 'bool':
        train_clean[col] = train_clean[col].astype(int)

train_clean.head()

train_clean.columns

df_train.head()

train_clean.head()

train_clean.dtypes

# # The list of columns you want to check for outliers
# outlier_columns = ['age_of_car', 'age_of_policyholder', 'population_density']

# # Create a new DataFrame to avoid changing the original
# train_clean_no_outliers = train_clean.copy()

# # Loop through each column to remove outliers
# for col in outlier_columns:
#     # Use .loc to find the row where the index matches the column name
#     lower_bound = stats.loc[col, 'Lower Bound']
#     upper_bound = stats.loc[col, 'Upper Bound']

#     # Filter the DataFrame
#     train_clean_no_outliers = train_clean_no_outliers[
#         (train_clean_no_outliers[col] >= lower_bound) &
#         (train_clean_no_outliers[col] <= upper_bound)
#     ]

# The list of columns you want to check for outliers
outlier_columns = ['age_of_car', 'age_of_policyholder', 'population_density']

# Create a new DataFrame to avoid changing the original
train_clean_no_outliers = train_clean.copy()

# Calculate the median for each column *before* addressing outliers
# This assumes 'train_clean' is the original, full dataset
median_values = train_clean[outlier_columns].median()

# Loop through each column to replace outliers with the median
for col in outlier_columns:
    # Get the pre-calculated bounds from the 'stats' DataFrame (e.g., Q1 - 1.5*IQR, Q3 + 1.5*IQR)
    lower_bound = stats.loc[col, 'Lower Bound']
    upper_bound = stats.loc[col, 'Upper Bound']

    # Get the median value for the current column
    col_median = median_values[col]

    # --- Step 1: Replace values below the lower bound with the median ---

    # Create a boolean mask for lower outliers
    lower_outliers_mask = train_clean_no_outliers[col] < lower_bound

    # Use .loc to replace the outlier values in the new DataFrame
    train_clean_no_outliers.loc[lower_outliers_mask, col] = col_median

    # --- Step 2: Replace values above the upper bound with the median ---

    # Create a boolean mask for upper outliers
    upper_outliers_mask = train_clean_no_outliers[col] > upper_bound

    # Use .loc to replace the outlier values in the new DataFrame
    train_clean_no_outliers.loc[upper_outliers_mask, col] = col_median

# train_clean_no_outliers kini berisi data yang outlier-nya sudah diganti dengan median

stats.head(20)

train_clean.columns

from sklearn.preprocessing import StandardScaler

# ‡∏™‡∏£‡πâ‡∏≤‡∏á list ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏™‡πÄ‡∏Å‡∏•
numeric_cols_to_scale = [
    'policy_tenure', 'age_of_car', 'age_of_policyholder', 'population_density',
    'make', 'airbags', 'displacement', 'cylinder',
    'turning_radius', 'length', 'width', 'height', 'gross_weight',
    'ncap_rating', 'Torque', 'Torque_rpm', 'Power', 'Power_rpm'
]

# ‡∏™‡∏£‡πâ‡∏≤‡∏á StandardScaler object
scaler = StandardScaler()

# ‡∏õ‡∏£‡∏±‡∏ö‡∏™‡πÄ‡∏Å‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô DataFrame
# ‡πÉ‡∏ä‡πâ .fit_transform() ‡∏Å‡∏±‡∏ö‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ outliers
train_clean_no_outliers[numeric_cols_to_scale] = scaler.fit_transform(
    train_clean_no_outliers[numeric_cols_to_scale]
)

train_clean_no_outliers.head()

train_clean_no_outliers.shape

claim_counts = train_clean_no_outliers['is_claim'].value_counts()

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
print(claim_counts)

train_clean_no_outliers.columns

train_clean_no_outliers.head()

# ‡πÅ‡∏õ‡∏•‡∏á dtypes ‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡∏î‡∏π‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
dtypes_df = train_clean_no_outliers.dtypes.reset_index()
dtypes_df.columns = ['Column', 'Data Type']
print(dtypes_df)

dtypes_df.head(100)

# ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå is_claim ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å DataFrame
is_claim_column = train_clean_no_outliers['is_claim']
train_clean_no_outliers = train_clean_no_outliers.drop(columns=['is_claim'])

# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå is_claim ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
train_clean_no_outliers['is_claim'] = is_claim_column

# ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
print(train_clean_no_outliers.columns)

"""# <h1>Feature Selection</h1><br>
<p>‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ SelectKBest ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ f_classif (ANOVA F-value) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ is_claim</p>
"""

from sklearn.feature_selection import SelectKBest, f_classif

# ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå (X) ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (y)
X = train_clean_no_outliers.drop(columns=['is_claim'])
y = train_clean_no_outliers['is_claim']

# ‡πÉ‡∏ä‡πâ SelectKBest ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå 20 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
# ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ k (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£) ‡πÑ‡∏î‡πâ
selector = SelectKBest(score_func=f_classif, k=20)
X_new = selector.fit_transform(X, y)

# ‡∏î‡∏π‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
selected_features_mask = selector.get_support()
selected_features = X.columns[selected_features_mask]

print("‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å:", list(selected_features))

"""# <h1>Feature Extraction</h1><br>
<p>‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ Principal Component Analysis (PCA) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÑ‡∏ß‡πâ‡πÑ‡∏î‡πâ</p>
"""

from sklearn.decomposition import PCA

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå
car_size_features = ['length', 'width', 'height', 'gross_weight', 'turning_radius']

# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
X_size = train_clean_no_outliers[car_size_features]

# ‡πÉ‡∏ä‡πâ PCA ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏°‡∏¥‡∏ï‡∏¥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏Å (Principal Components)
pca = PCA(n_components=2)
principal_components = pca.fit_transform(X_size)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏Å (Principal Components)
pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])

print("DataFrame ‡∏ó‡∏µ‡πà‡∏•‡∏î‡∏°‡∏¥‡∏ï‡∏¥‡πÅ‡∏•‡πâ‡∏ß (PCA):")
print(pca_df.head())

# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏≤‡∏Å SelectKBest
X_selected = X[selected_features]

# ‡∏£‡∏ß‡∏° DataFrame ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å SelectKBest ‡πÅ‡∏•‡∏∞ PCA ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
# ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á DataFrame ‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô
final_X = pd.concat([X_selected.reset_index(drop=True), pca_df.reset_index(drop=True)], axis=1)

# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå is_claim ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
final_df = pd.concat([final_X, y.reset_index(drop=True)], axis=1)

print("DataFrame ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•:")
print(final_df.head())
print("\n‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:", final_df.shape[1])
print("‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå:", final_df.columns)

"""# <h1>Training Model

# <h2>SMOTE
"""

from imblearn.over_sampling import SMOTE
from collections import Counter
from sklearn.model_selection import train_test_split

# 1. ‡πÅ‡∏¢‡∏Å‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå (X) ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (y)
X = final_df.drop(columns=['is_claim'])
y = final_df['is_claim']

# 2. ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∏‡∏î‡∏ù‡∏∂‡∏Å (70%) ‡πÅ‡∏•‡∏∞‡∏ä‡∏∏‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏ù‡∏∂‡∏Å‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
print("‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏ù‡∏∂‡∏Å‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô:", Counter(y_train))

# 3. ‡πÉ‡∏ä‡πâ SMOTE ‡∏Å‡∏±‡∏ö‡∏ä‡∏∏‡∏î‡∏ù‡∏∂‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
smote = SMOTE(random_state=17)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥ SMOTE
print("‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏ù‡∏∂‡∏Å‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥ SMOTE:", Counter(y_train_resampled))

"""# Build Model"""

import time
from sklearn.preprocessing import StandardScaler

"""<h1>Logistic Regression Implementation

1. Standard Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, recall_score,precision_score,roc_curve, auc, roc_auc_score
start = time.time()
log_reg = LogisticRegression(max_iter=2000, solver='liblinear')
log_reg.fit(X_train_resampled, y_train_resampled)
end = time.time()
train_time_log_reg = end - start

y_pred = log_reg.predict(X_test)
acc_log_reg = accuracy_score(y_test, y_pred)
pre_log_reg = precision_score(y_test, y_pred)
recall_log_reg = recall_score(y_test, y_pred)
print("  Training Time: %.4f seconds" % train_time_log_reg)
print("  Accuracy: %.4f" % acc_log_reg)
print("Precision: %.4f" %pre_log_reg)
print("Recall: %.4f" % recall_log_reg)
print(classification_report(y_test, y_pred))

"""2. Polynomial Logistic Regression

---


"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import time

# Create polynomial features as before
poly = PolynomialFeatures(degree=2, include_bias=False)
X_train_poly = poly.fit_transform(X_train_resampled)
X_test_poly = poly.transform(X_test)

# No scaling is performed here. The scaler and the scaled variables are removed.

# Train the model on the unscaled polynomial features
start = time.time()
log_reg_poly = LogisticRegression(max_iter=2000, solver='lbfgs')
log_reg_poly.fit(X_train_poly, y_train_resampled)
end = time.time()
train_time_log_poly = end - start

# Make predictions on the unscaled polynomial test features
y_pred_poly = log_reg_poly.predict(X_test_poly)

# Evaluate the model
acc_log_poly = accuracy_score(y_test, y_pred_poly)
pre_log_poly = precision_score(y_test, y_pred_poly)
recall_log_poly = recall_score(y_test, y_pred_poly)

print("  Training Time: %.4f seconds" % train_time_log_poly)
print("  Accuracy: %.4f" % acc_log_poly)
print("Precision: %.4f" %pre_log_poly)
print("Recall: %.4f" % recall_log_poly)
print(classification_report(y_test, y_pred_poly))

"""<H1>Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
start = time.time()
dt_model = DecisionTreeClassifier(random_state=17)
dt_model.fit(X_train_resampled, y_train_resampled)
end = time.time()
train_time_dt = end - start

y_pred_dt = dt_model.predict(X_test)
y_pred_proba_dt = dt_model.predict_proba(X_test)[:, 1]

acc_dt = accuracy_score(y_test, y_pred_dt)
pre_dt = precision_score(y_test, y_pred_dt)
recall_dt = recall_score(y_test, y_pred_dt)

print(" Training Time: %.4f seconds" % train_time_dt)
print(" Accuracy: %.4f" % acc_dt)
print(" Precision: %.4f" % pre_dt)
print(" Recall: %.4f" % recall_dt)
print(classification_report(y_test, y_pred_dt))

"""<h1>Random Forest"""

from sklearn.ensemble import RandomForestClassifier
start = time.time()
rf_model = RandomForestClassifier(n_estimators=100, random_state=17, n_jobs=-1)
rf_model.fit(X_train_resampled, y_train_resampled)
end = time.time()
train_time_rf = end - start

y_pred_rf = rf_model.predict(X_test)
y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]

acc_rf = accuracy_score(y_test, y_pred_rf)
pre_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)

print(" Training Time: %.4f seconds" % train_time_rf)
print(" Accuracy: %.4f" % acc_rf)
print(" Precision: %.4f" % pre_rf)
print(" Recall: %.4f" % recall_rf)
print(classification_report(y_test, y_pred_rf))

"""<h1>KNN"""

from sklearn.neighbors import KNeighborsClassifier
start = time.time()
knn_model = KNeighborsClassifier(n_neighbors=5, algorithm='auto', n_jobs=-1)
knn_model.fit(X_train_resampled, y_train_resampled)
end = time.time()
train_time_knn = end - start

y_pred_knn = knn_model.predict(X_test)
y_pred_proba_knn = knn_model.predict_proba(X_test)[:, 1]

acc_knn = accuracy_score(y_test, y_pred_knn)
pre_knn = precision_score(y_test, y_pred_knn)
recall_knn = recall_score(y_test, y_pred_knn)

print(" Training Time: %.4f seconds" % train_time_knn)
print(" Accuracy: %.4f" % acc_knn)
print(" Precision: %.4f" % pre_knn)
print(" Recall: %.4f" % recall_knn)
print(classification_report(y_test, y_pred_knn))

"""<h1>Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
start = time.time()
nb_model = GaussianNB()
nb_model.fit(X_train_resampled, y_train_resampled)
end = time.time()
train_time_nb = end - start

y_pred_nb = nb_model.predict(X_test)
y_pred_proba_nb = nb_model.predict_proba(X_test)[:, 1]

acc_nb = accuracy_score(y_test, y_pred_nb)
pre_nb = precision_score(y_test, y_pred_nb)
recall_nb = recall_score(y_test, y_pred_nb)


print(" Training Time: %.4f seconds" % train_time_nb)
print(" Accuracy: %.4f" % acc_nb)
print(" Precision: %.4f" % pre_nb)
print(" Recall: %.4f" % recall_nb)
print(classification_report(y_test, y_pred_nb))

"""<h1>SVM Implementation"""

from sklearn.svm import SVC

"""1. Linear (kernel='linear')"""

# Linear
start = time.time()
svm_linear = SVC(kernel='linear', C=1, random_state=17)
svm_linear.fit(X_train_resampled, y_train_resampled)
y_pred_svm_lin = svm_linear.predict(X_test)
end = time.time()

train_time_svm_linear = end - start
acc_svm_linear = accuracy_score(y_test, y_pred_svm_lin)

print("SVM Linear Accuracy %.4f " % acc_svm_linear)
print("  Training Time: %.4f seconds" % train_time_svm_linear)
print(classification_report(y_test, y_pred_svm_lin))

"""2. Polynomial (kernel='poly')"""

start = time.time()
svm_poly = SVC(kernel='poly', degree=3)
svm_poly.fit(X_train_resampled, y_train_resampled)
y_pred_svm_poly = svm_poly.predict(X_test)
end = time.time()

train_time_svm_poly = end - start
acc_svm_poly = accuracy_score(y_test, y_pred_svm_poly)

print("SVM Polynomial Accuracy %.4f " % acc_svm_poly)
print("  Training Time: %.4f seconds" % train_time_svm_poly)
print(classification_report(y_test, y_pred_svm_poly))

""" 3. RBF (kernel='rbf')"""

start = time.time()
svm_rbf = SVC(kernel='rbf', C=1, gamma='scale', random_state=17)
svm_rbf.fit(X_train_resampled, y_train_resampled)
y_pred_svm_rbf = svm_rbf.predict(X_test)
end = time.time()

train_time_svm_rbf = end - start
acc_svm_rbf = accuracy_score(y_test, y_pred_svm_rbf)

print("SVM RBF Accuracy %.4f" % acc_svm_rbf)
print("  Training Time: %.4f seconds" % train_time_svm_rbf)
print(classification_report(y_test, y_pred_svm_rbf))

"""# Performance Model

<h1> Table
"""

import pandas as pd
from tabulate import tabulate

results = {
    "Model": ["Logistic Regression(Standard)", "Logistic Regression(Polynomial)","SVM (Linear)", "SVM (Polynomial)", "SVM (RBF)"],
    "Training Time (s)": [train_time_log_reg, train_time_log_poly, train_time_svm_linear, train_time_svm_poly, train_time_svm_rbf],
    "Accuracy": [acc_log_reg, acc_log_poly , acc_svm_linear, acc_svm_poly, acc_svm_rbf]
}

df_results = pd.DataFrame(results)

print(tabulate(df_results, headers="keys", tablefmt="pretty", showindex=True))

"""<h1>Bar Chart 1: Accuracy of each model"""

import matplotlib.pyplot as plt

plt.figure(figsize=(5,3))
plt.bar(df_results["Model"], df_results["Accuracy"])
plt.title("Accuracy Comparison")
plt.ylabel("Accuracy")
plt.xticks(rotation=20, ha="right")
plt.show()

"""<h1>Bar Chart 2: Training Time of each model"""

plt.figure(figsize=(5,3))
plt.bar(df_results["Model"], df_results["Training Time (s)"], color="orange")
plt.title("Training Time Comparison")
plt.ylabel("Time (s)")
plt.xticks(rotation=20, ha="right")
plt.show()

"""<h1>Combined Chart: Accuracy vs Training Time"""

fig, ax1 = plt.subplots(figsize=(8,5))

ax2 = ax1.twinx()
ax1.bar(df_results["Model"], df_results["Training Time (s)"], color="lightblue", label="Training Time (s)")
ax2.plot(df_results["Model"], df_results["Accuracy"], color="green", marker="o", label="Accuracy")

ax1.set_xlabel("Model")
ax1.set_ylabel("Training Time (s)")
ax2.set_ylabel("Accuracy")

plt.title("Model Performance: Accuracy vs Training Time")
ax1.tick_params(axis="x", rotation=20)

fig.tight_layout()
plt.show()

results_extended = {
    "Model": [
        "Logistic Regression(Standard)", "Logistic Regression(Polynomial)",
        "SVM (Linear)", "SVM (Polynomial)", "SVM (RBF)",
        "Decision Tree", "Random Forest", "KNN", "Naive Bayes"
    ],
    "Training Time (s)": [
        train_time_log_reg, train_time_log_poly, train_time_svm_linear,
        train_time_svm_poly, train_time_svm_rbf,
        train_time_dt, train_time_rf, train_time_knn, train_time_nb
    ],
    "Accuracy": [
        acc_log_reg, acc_log_poly, acc_svm_linear, acc_svm_poly, acc_svm_rbf,
        acc_dt, acc_rf, acc_knn, acc_nb
    ]
}

df_results_extended = pd.DataFrame(results_extended)

print("\n" + "="*60)
print("‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
print("="*60)
print(tabulate(df_results_extended, headers="keys", tablefmt="pretty", showindex=True))

# ‡∏Å‡∏£‡∏≤‡∏ü Accuracy Comparison
plt.figure(figsize=(10,6))
plt.bar(df_results_extended["Model"], df_results_extended["Accuracy"])
plt.title("Accuracy Comparison")
plt.ylabel("Accuracy")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()

# ‡∏Å‡∏£‡∏≤‡∏ü Training Time Comparison
plt.figure(figsize=(10,6))
plt.bar(df_results_extended["Model"], df_results_extended["Training Time (s)"], color="orange")
plt.title("Training Time Comparison")
plt.ylabel("Time (s)")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()

# ‡∏Å‡∏£‡∏≤‡∏ü Combined
fig, ax1 = plt.subplots(figsize=(12,7))

ax2 = ax1.twinx()
ax1.bar(df_results_extended["Model"], df_results_extended["Training Time (s)"], color="lightblue", alpha=0.6, label="Training Time (s)")
ax2.plot(df_results_extended["Model"], df_results_extended["Accuracy"], color="green", marker="o", linestyle='--', label="Accuracy")

ax1.set_xlabel("Model (‡πÇ‡∏°‡πÄ‡∏î‡∏•)")
ax1.set_ylabel("Training Time (s)", color='lightblue')
ax2.set_ylabel("Accuracy", color='green')

plt.title("Model Performance: Accuracy vs Training Time")
ax1.tick_params(axis="x", rotation=45)

fig.tight_layout()
plt.show()

# 8.1 ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô (Predict Probabilities) ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏¥‡∏°
# Logistic Regression (Standard)
y_pred_proba_log_reg = log_reg.predict_proba(X_test)[:, 1]

# Logistic Regression (Polynomial) - ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ X_test_poly
y_pred_proba_log_poly = log_reg_poly.predict_proba(X_test_poly)[:, 1]

# 8.2 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SVM: ‡∏ï‡πâ‡∏≠‡∏á‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà‡πÇ‡∏î‡∏¢‡πÉ‡∏™‡πà probability=True ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô
#      ‡πÅ‡∏•‡∏∞‡∏ô‡∏≥‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏≤‡∏ü ROC ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥/‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏î‡∏¥‡∏°

# SVM (Linear) - ROC
svm_linear_roc = SVC(kernel='linear', C=1, random_state=17, probability=True)
svm_linear_roc.fit(X_train_resampled, y_train_resampled)
y_pred_proba_svm_lin = svm_linear_roc.predict_proba(X_test)[:, 1]

# SVM (Polynomial) - ROC
svm_poly_roc = SVC(kernel='poly', degree=3, probability=True)
svm_poly_roc.fit(X_train_resampled, y_train_resampled)
y_pred_proba_svm_poly = svm_poly_roc.predict_proba(X_test)[:, 1]

# SVM (RBF) - ROC
svm_rbf_roc = SVC(kernel='rbf', C=1, gamma='scale', random_state=17, probability=True)
svm_rbf_roc.fit(X_train_resampled, y_train_resampled)
y_pred_proba_svm_rbf = svm_rbf_roc.predict_proba(X_test)[:, 1]


# 8.3 ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
models_proba = {
    "LogReg (Standard)": y_pred_proba_log_reg,
    "LogReg (Polynomial)": y_pred_proba_log_poly,
    "SVM (Linear)": y_pred_proba_svm_lin,
    "SVM (Polynomial)": y_pred_proba_svm_poly,
    "SVM (RBF)": y_pred_proba_svm_rbf,
    "Decision Tree": y_pred_proba_dt,
    "Random Forest": y_pred_proba_rf,
    "KNN": y_pred_proba_knn,
    "Naive Bayes": y_pred_proba_nb
}

plt.figure(figsize=(10, 8))
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess (AUC = 0.50)')

for name, y_proba in models_proba.items():
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ROC curve ‡πÅ‡∏•‡∏∞ AUC
    fpr, tpr, thresholds = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)

    # ‡∏û‡∏•‡πá‡∏≠‡∏ï‡πÄ‡∏™‡πâ‡∏ô ROC
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.4f})')

plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR) / Recall')
plt.title('Receiver Operating Characteristic (ROC) Curve Comparison')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print("\nFinished")

"""<br>

---

<h1>üîÅ ‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡∏à‡∏≤‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏°‡∏¥‡∏î‡πÄ‡∏ó‡∏≠‡∏° (Post-Midterm Continuation)</h1>

# üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πà‡∏≤‡∏á‡πÜ Baseline ‡∏à‡∏≤‡∏Å‡∏≠‡∏±‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô

---

## 1. Decision Tree üå≥

### Hyperparameters
- `random_state = 17`
- `criterion = 'gini'`
- `max_depth = None`

### Performance
- **Accuracy:** 0.8451
- **Precision:** 0.0891
- **Recall:** 0.1472
- **Training Time:** 1.3585 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‚è±Ô∏è

### Classification Report
| | precision | recall | f1-score | support |
|:---|---:|---:|---:|---:|
| **0** | 0.94 | 0.89 | 0.92 | 16423 |
| **1** | 0.09 | 0.15 | 0.11 | 1155 |
| | | | | |
| **accuracy** | | | 0.85 | 17578 |
| **macro avg** | 0.51 | 0.52 | 0.51 | 17578 |
| **weighted avg** | 0.88 | 0.85 | 0.86 | 17578 |

---

## 2. Standard Logistic Regression

### Hyperparameters
- `max_iter = 2000`
- `solver = 'liblinear'`

### Performance
- **Accuracy:** 0.5659
- **Precision:** 0.0857
- **Recall:** 0.5801
- **Training Time:** 0.9110 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‚è±Ô∏è

### Classification Report
| | precision | recall | f1-score | support |
|:---|---:|---:|---:|---:|
| **0** | 0.95 | 0.56 | 0.71 | 16423 |
| **1** | 0.09 | 0.58 | 0.15 | 1155 |
| | | | | |
| **accuracy** | | | 0.57 | 17578 |
| **macro avg** | 0.52 | 0.57 | 0.43 | 17578 |
| **weighted avg** | 0.89 | 0.57 | 0.67 | 17578 |

---

## 3. Polynomial Logistic Regression

### Hyperparameters
- `max_iter = 2000`
- `solver = 'lbfgs'`
- `degree = 2`
- `include_bias = False`

### Performance
- **Accuracy:** 0.5396
- **Precision:** 0.0848
- **Recall:** 0.6139
- **Training Time:** 45.3632 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‚è±Ô∏è

### Classification Report
| | precision | recall | f1-score | support |
|:---|---:|---:|---:|---:|
| **0** | 0.95 | 0.53 | 0.68 | 16423 |
| **1** | 0.08 | 0.61 | 0.15 | 1155 |
| | | | | |
| **accuracy** | | | 0.54 | 17578 |
| **macro avg** | 0.52 | 0.57 | 0.42 | 17578 |
| **weighted avg** | 0.89 | 0.54 | 0.65 | 17578 |

---

## 4. SVM (Linear Kernel)

### Hyperparameters
- `kernel='linear'`
- `C = 1`

### Performance
- **Accuracy:** 0.5430
- **Training Time:** 588.6491 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‚è±Ô∏è

### Classification Report
| | precision | recall | f1-score | support |
|:---|---:|---:|---:|---:|
| **0** | 0.95 | 0.54 | 0.69 | 16423 |
| **1** | 0.09 | 0.63 | 0.15 | 1155 |
| **weighted avg** | 0.90 | 0.54 | 0.65 | 17578 |

---

## 5. SVM (Polynomial Kernel)

### Hyperparameters
- `kernel='poly'`
- `degree=3`

### Performance
- **Accuracy:** 0.4895
- **Training Time:** 553.6186 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‚è±Ô∏è

### Classification Report
| | precision | recall | f1-score | support |
|:---|---:|---:|---:|---:|
| **0** | 0.96 | 0.48 | 0.64 | 16423 |
| **1** | 0.08 | 0.68 | 0.15 | 1155 |
| **weighted avg** | 0.90 | 0.49 | 0.60 | 17578 |

---

## 6. SVM (RBF Kernel)

### Hyperparameters
- `kernel='rbf'`
- `C = 1`
- `gamma='scale'`

### Performance
- **Accuracy:** 0.4944
- **Training Time:** 858.9032 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‚è±Ô∏è

### Classification Report
| | precision | recall | f1-score | support |
|:---|---:|---:|---:|---:|
| **0** | 0.96 | 0.48 | 0.64 | 16423 |
| **1** | 0.09 | 0.69 | 0.15 | 1155 |
| **weighted avg** | 0.90 | 0.49 | 0.61 | 17578 |

---

## 7. Gaussian Naive Bayes

### Hyperparameters
- `GaussianNB()`

### Performance
- **Accuracy:** 0.2628
- **Precision:** 0.0695
- **Recall:** 0.8251
- **Training Time:** 0.0726 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‚è±Ô∏è

### Classification Report
| | precision | recall | f1-score | support |
|:---|---:|---:|---:|---:|
| **0** | 0.95 | 0.22 | 0.36 | 16423 |
| **1** | 0.07 | 0.83 | 0.13 | 1155 |
| **weighted avg** | 0.89 | 0.26 | 0.35 | 17578 |

---

## 8. K-Nearest Neighbors (KNN)  voisinage

### Hyperparameters
- `n_neighbors = 5`
- `algorithm = 'auto'`
- `n_jobs = -1`

### Performance
- **Accuracy:** 0.7044
- **Precision:** 0.0763
- **Recall:** 0.3152
- **Training Time:** 0.0333 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‚è±Ô∏è

### Classification Report
| | precision | recall | f1-score | support |
|:---|---:|---:|---:|---:|
| **0** | 0.94 | 0.73 | 0.82 | 16423 |
| **1** | 0.08 | 0.32 | 0.12 | 1155 |
| **weighted avg** | 0.88 | 0.70 | 0.78 | 17578 |

<h2>üß© ‡∏ö‡∏ó‡∏ô‡∏≥</h2>
<p>
‡∏™‡πà‡∏ß‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô <b>Baseline ‡∏Ç‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô</b> ‡∏ã‡∏∂‡πà‡∏á‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô 5 ‡∏ï‡∏±‡∏ß ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà  
<b>Decision Tree, Logistic Regression, SVM, Naive Bayes ‡πÅ‡∏•‡∏∞ KNN</b>  
‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Preprocessing) ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Ñ‡∏£‡∏ö‡πÅ‡∏•‡πâ‡∏ß ‡πÇ‡∏î‡∏¢‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÑ‡∏õ ‡∏Ñ‡∏∑‡∏≠ <b>‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡∏Ç‡∏≠‡∏á‡∏ó‡∏µ‡∏°‡πÄ‡∏£‡∏≤</b>  
‡πÄ‡∏û‡∏∑‡πà‡∏≠ ‚Äú‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‚Äù ‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏Å‡∏≥‡∏´‡∏ô‡∏î
</p>

---

<h2>üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏Å‡∏≥‡∏´‡∏ô‡∏î</h2>
<ul>
  <li>‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ baseline</li>
  <li>‡∏ó‡∏î‡∏•‡∏≠‡∏á <b>preprocessing ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà</b></li>
  <li>‡∏õ‡∏£‡∏±‡∏ö <b>hyperparameter</b> ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö</li>
  <li>‡∏ó‡∏î‡∏•‡∏≠‡∏á <b>ensemble</b> ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)</li>
  <li>‡πÄ‡∏ô‡πâ‡∏ô <b>‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ (journey)</b> ‡πÅ‡∏•‡∏∞ <b>‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πà‡∏á‡πÉ‡∏™</b></li>
</ul>

</ul>

---

<h2>‚öôÔ∏è ‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á (Experiment Plan)</h2>

<h3>1. Preprocessing optional</h3>
<ul>
  <li>‡πÉ‡∏ä‡πâ <b>StandardScaler</b> ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏ß‡∏ï‡πà‡∏≠‡∏™‡πÄ‡∏Å‡∏• ‡πÄ‡∏ä‡πà‡∏ô Logistic Regression, SVM, KNN</li>
  <li>‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á <b>SMOTE</b> ‡∏Å‡∏±‡∏ö <b>class_weight='balanced'</b></li>
  <li>‡∏à‡∏≥‡∏Å‡∏±‡∏î <b>Polynomial feature</b> ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (numeric only + interaction only)</li>
  <li>‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£ <b>‡∏£‡∏±‡πà‡∏ß‡πÑ‡∏´‡∏•‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (data leakage)</b> ‡πÇ‡∏î‡∏¢‡∏ó‡∏≥ preprocessing ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Pipeline ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô</li>
</ul>

1.1 ‡πÉ‡∏ä‡πâ StandardScaler ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏ß‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡πà‡∏≠‡∏™‡πÄ‡∏Å‡∏• (LogReg / SVM / KNN)
"""

# ==== 1) StandardScaler + Pipeline (‡∏Å‡∏±‡∏ô leakage) ====
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score

def eval_model(name, model, X_tr, y_tr, X_te, y_te):
    model.fit(X_tr, y_tr)
    y_pred = model.predict(X_te)
    print(f"\n=== {name} ===")
    print(f"Accuracy : {accuracy_score(y_te, y_pred):.4f}")
    print(f"Precision: {precision_score(y_te, y_pred, zero_division=0):.4f}")
    print(f"Recall   : {recall_score(y_te, y_pred, zero_division=0):.4f}")
    print(classification_report(y_te, y_pred, digits=4))

# Logistic Regression + scaling
logreg_scaled = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", LogisticRegression(max_iter=2000, solver="liblinear"))
])
eval_model("LogReg + StandardScaler", logreg_scaled, X_train, y_train, X_test, y_test)

# SVM (RBF) + scaling
svm_scaled = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", SVC(kernel="rbf", C=1.0, gamma="scale"))
])
eval_model("SVM-RBF + StandardScaler", svm_scaled, X_train, y_train, X_test, y_test)

# KNN + scaling
knn_scaled = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", KNeighborsClassifier(n_neighbors=5))
])
eval_model("KNN(k=5) + StandardScaler", knn_scaled, X_train, y_train, X_test, y_test)

"""1.2 ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö SMOTE vs class_weight='balanced'

‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô SMOTE: ‡πÉ‡∏ä‡πâ imblearn.pipeline.Pipeline ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ SMOTE ‡∏ó‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞ train (‡∏Å‡∏±‡∏ô leakage)

‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô class_weight: ‡πÑ‡∏°‡πà oversample ‡πÅ‡∏ï‡πà‡∏ä‡∏î‡πÄ‡∏ä‡∏¢‡∏î‡πâ‡∏ß‡∏¢ weight
"""

# ==== 2) SMOTE vs class_weight ====
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.linear_model import LogisticRegression

# (A) Logistic Regression + SMOTE + Scaling
logreg_smote = ImbPipeline(steps=[
    ("smote", SMOTE(random_state=17)),
    ("scaler", StandardScaler()),
    ("clf", LogisticRegression(max_iter=2000, solver="liblinear"))
])
eval_model("LogReg + SMOTE + Scaling", logreg_smote, X_train, y_train, X_test, y_test)

# (B) Logistic Regression + class_weight (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ SMOTE)
logreg_weighted = Pipeline(steps=[
    ("scaler", StandardScaler()),
    ("clf", LogisticRegression(max_iter=2000, solver="liblinear",
                               class_weight="balanced"))
])
eval_model("LogReg + class_weight(balanced) + Scaling", logreg_weighted, X_train, y_train, X_test, y_test)

"""**‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å/‡∏ä‡πâ‡∏≤ ‡∏•‡∏≠‡∏á‡∏•‡∏î SMOTE ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ sampling_strategy=0.5 (‡πÉ‡∏´‡πâ minority ‡∏´‡∏•‡∏±‡∏á oversample ~50% ‡∏Ç‡∏≠‡∏á majority) ‡πÄ‡∏ä‡πà‡∏ô:
SMOTE(random_state=17, sampling_strategy=0.5)**

1.3 Polynomial numeric-only + interaction-only

‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏≠‡∏á (‡∏•‡∏î feature ‡∏£‡∏∞‡πÄ‡∏ö‡∏¥‡∏î) ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (‡∏Å‡∏£‡∏ì‡∏µ X ‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå non-numeric ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ numeric ‡∏Å‡πà‡∏≠‡∏ô)
"""

# ==== 3) Polynomial (numeric-only + interaction-only) ====
from sklearn.preprocessing import PolynomialFeatures
import numpy as np
import pandas as pd

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ numeric columns (‡∏Å‡∏£‡∏ì‡∏µ X ‡πÄ‡∏õ‡πá‡∏ô DataFrame)
if isinstance(X_train, pd.DataFrame):
    num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()
    Xtr_num = X_train[num_cols]
    Xte_num = X_test[num_cols]
else:
    # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô numpy ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤ numeric ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    Xtr_num = X_train
    Xte_num = X_test

poly_interaction_pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("poly", PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),
    ("clf", LogisticRegression(max_iter=2000, solver="saga"))
])
eval_model("LogReg + Interaction-only Poly(d=2) + Scaling",
           poly_interaction_pipe, Xtr_num, y_train, Xte_num, y_test)

"""1.4 ‡∏ó‡∏≥‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á ‚Äú‡πÉ‡∏ô Pipeline‚Äù ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô data leakage

‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡πä‡∏î‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ñ‡∏π‡∏Å‡∏´‡πà‡∏≠‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Pipeline/ImbPipeline ‡πÅ‡∏•‡πâ‡∏ß ‚Äî ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ point ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ preprocessing ‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å train ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡∏ô‡∏≥‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö test ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á

‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö SVM/KNN ‡∏î‡πâ‡∏ß‡∏¢ SMOTE ‡∏Å‡πá‡πÅ‡∏Ñ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏£‡∏á clf ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ô‡∏±‡πâ‡∏ô ‡πÜ:
"""

svm_smote = ImbPipeline(steps=[
    ("smote", SMOTE(random_state=17)),
    ("scaler", StandardScaler()),
    ("clf", SVC(kernel="rbf", C=1.0, gamma="scale"))
])
eval_model("SVM-RBF + SMOTE + Scaling", svm_smote, X_train, y_train, X_test, y_test)

"""<h2>üß© ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô (Baseline) ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1</h2>

<table>
  <thead>
    <tr>
      <th>‡πÇ‡∏°‡πÄ‡∏î‡∏•</th>
      <th>‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ</th>
      <th>Accuracy</th>
      <th>Precision (class 1)</th>
      <th>Recall (class 1)</th>
      <th>‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><b>Logistic Regression (baseline)</b></td>
      <td>‡πÑ‡∏°‡πà‡∏°‡∏µ Scaler</td>
      <td>0.5660</td>
      <td>0.0856</td>
      <td>0.578</td>
      <td>‚úÖ ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ</td>
    </tr>
    <tr>
      <td><b>LogReg + SMOTE + Scaling</b></td>
      <td>Pipeline, scaled</td>
      <td>0.5662</td>
      <td>0.0856</td>
      <td>0.578</td>
      <td>üîπ ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á baseline ‚Üí ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á</td>
    </tr>
    <tr>
      <td><b>LogReg + class_weight(balanced)</b></td>
      <td>‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ SMOTE</td>
      <td>0.5636</td>
      <td>0.0882</td>
      <td>0.604</td>
      <td>üîπ Recall ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡∏∂‡πà‡∏á (+0.02)</td>
    </tr>
    <tr>
      <td><b>SVM-RBF + SMOTE + Scaling</b></td>
      <td>Pipeline ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô leakage</td>
      <td>0.4967</td>
      <td>0.0843</td>
      <td><b>0.6753</b></td>
      <td>üî• Recall ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ä‡∏±‡∏î (‡∏à‡∏≤‡∏Å ~0.63 ‚Üí 0.68)</td>
    </tr>
    <tr>
      <td><b>Decision Tree (baseline)</b></td>
      <td>criterion=gini, max_depth=None</td>
      <td>0.845</td>
      <td>0.0891</td>
      <td>0.147</td>
      <td>‚Äì</td>
    </tr>
    <tr>
      <td><b>KNN (scaled)</b></td>
      <td>StandardScaler</td>
      <td>0.931</td>
      <td>0.1233</td>
      <td>0.0078</td>
      <td>üîª Accuracy ‡∏™‡∏π‡∏á‡πÅ‡∏ï‡πà Recall ‡∏ï‡πà‡∏≥ (‡∏õ‡∏Å‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á imbalance)</td>
    </tr>
  </tbody>
</table>

---

<h3>üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°</h3>

- ‡πÉ‡∏ä‡πâ <b>Pipeline + Scaling</b> ‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î data leakage  
- <b>class_weight='balanced'</b> ‡πÉ‡∏´‡πâ Recall ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á oversample  
- <b>SVM-RBF + SMOTE + Scaling</b> ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (Recall ‚âà 0.6753)  
- Precision ‡∏¢‡∏±‡∏á‡∏ï‡πà‡∏≥‡πÅ‡∏ï‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ô‡πâ‡∏ô‡∏à‡∏±‡∏ö‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á (‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ñ‡∏•‡∏°‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô)  
- ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏ñ‡∏±‡∏î‡πÑ‡∏õ: ‡∏ó‡∏î‡∏•‡∏≠‡∏á <b>Ensemble / XGBoost</b> ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏° Precision ‡πÇ‡∏î‡∏¢‡∏Ñ‡∏á Recall ‡πÑ‡∏ß‡πâ

---

‚úÖ <b>‡∏™‡∏£‡∏∏‡∏õ:</b> ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏á‡∏°‡∏¥‡∏î‡πÄ‡∏ó‡∏≠‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏û‡∏¥‡πà‡∏° Recall ‡∏à‡∏≤‡∏Å ~0.57 ‚Üí ~0.68  
‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏ô‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡∏∞‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå

<br>

<h3>2. Hyperparameter Tuning</h3>
<p>‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÄ‡∏ä‡πà‡∏ô</p>
<ul>
  <li><b>Logistic Regression</b> ‚Üí C, penalty, solver, class_weight</li>
  <li><b>SVM (RBF)</b> ‚Üí C, gamma, class_weight</li>
  <li><b>KNN</b> ‚Üí n_neighbors, weights, metric</li>
  <li><b>Decision Tree</b> ‚Üí max_depth, min_samples_split, class_weight</li>
</ul>
"""

# ==== ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GridSearch ====
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline as ImbPipeline  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏™‡πà SMOTE ‡πÉ‡∏ô CV
from imblearn.over_sampling import SMOTE

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

import numpy as np
import time

SEED = 17
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)

# ‡πÄ‡∏£‡∏≤‡πÇ‡∏ü‡∏Å‡∏±‡∏™‡∏á‡∏≤‡∏ô imbalance ‚Üí ‡∏£‡∏µ‡πÄ‡∏ü‡∏¥‡∏ï‡∏î‡πâ‡∏ß‡∏¢ "recall" (‡∏à‡∏±‡∏ö‡∏Ñ‡∏•‡∏≤‡∏™ 1 ‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô)
scoring = {
    "acc": "accuracy",
    "precision": make_scorer(precision_score, zero_division=0),
    "recall": "recall",
    "f1": "f1"
}
REFIT = "recall"

def report_best(name, gs, X_test, y_test):
    print(f"\n=== {name} ===")
    print("Best params:", gs.best_params_)
    print("CV best (refit metric: %s): %.4f" % (REFIT, gs.best_score_))
    y_pred = gs.best_estimator_.predict(X_test)
    print("Test Accuracy:  %.4f" % accuracy_score(y_test, y_pred))
    print("Test Precision: %.4f" % precision_score(y_test, y_pred, zero_division=0))
    print("Test Recall:    %.4f" % recall_score(y_test, y_pred, zero_division=0))
    print(classification_report(y_test, y_pred, digits=4))

"""2.1: ‡πÅ‡∏ö‡∏ö‡πÉ‡∏ä‡πâ class_weight='balanced' (‡πÑ‡∏°‡πà‡∏ó‡∏≥ SMOTE)

‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏±‡∏ô‡πÄ‡∏£‡πá‡∏ß ‡πÜ ‡πÅ‡∏•‡∏∞‡∏Å‡∏±‡∏ô overfitting ‡∏à‡∏≤‡∏Å oversampling :‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô baseline ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏π‡∏ô
"""

# ============ Logistic Regression (balanced) ============
pipe_lr_bal = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", LogisticRegression(max_iter=2000, class_weight="balanced", random_state=SEED))
])

param_lr_bal = {
    "clf__C": [0.1, 1, 5],
    "clf__solver": ["liblinear", "saga"],   # saga ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πÄ‡∏Å‡∏•‡πÉ‡∏´‡∏ç‡πà‡πÑ‡∏î‡πâ‡∏î‡∏µ
    "clf__penalty": ["l2"]                  # (‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ saga ‡∏à‡∏∞‡∏•‡∏≠‡∏á 'l1' ‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏ß‡∏¢ ‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö liblinear ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏∏‡∏Å‡∏Å‡∏£‡∏ì‡∏µ)
}

gs_lr_bal = GridSearchCV(
    pipe_lr_bal, param_lr_bal, cv=cv, scoring=scoring, refit=REFIT, n_jobs=-1, verbose=1
)
t0=time.time(); gs_lr_bal.fit(X_train, y_train); t1=time.time()
print("LR(balanced) grid time: %.2fs" % (t1-t0))
report_best("LogReg (balanced + scaling)", gs_lr_bal, X_test, y_test)

# ============ SVM (RBF, balanced) ============
pipe_svm_bal = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", SVC(kernel="rbf", class_weight="balanced", probability=False, random_state=SEED))
])

param_svm_bal = {
    "clf__C": [0.5, 1, 5],
    "clf__gamma": ["scale", 0.01, 0.1]
}

gs_svm_bal = GridSearchCV(
    pipe_svm_bal, param_svm_bal, cv=cv, scoring=scoring, refit=REFIT, n_jobs=-1, verbose=1
)
t0=time.time(); gs_svm_bal.fit(X_train, y_train); t1=time.time()
print("SVM(balanced) grid time: %.2fs" % (t1-t0))
report_best("SVM-RBF (balanced + scaling)", gs_svm_bal, X_test, y_test)

# ============ KNN (scaled) ============
pipe_knn = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", KNeighborsClassifier())
])

param_knn = {
    "clf__n_neighbors": [3, 5, 7, 11],
    "clf__weights": ["uniform", "distance"],
    "clf__metric": ["euclidean", "manhattan"]
}

gs_knn = GridSearchCV(
    pipe_knn, param_knn, cv=cv, scoring=scoring, refit=REFIT, n_jobs=-1, verbose=1
)
t0=time.time(); gs_knn.fit(X_train, y_train); t1=time.time()
print("KNN grid time: %.2fs" % (t1-t0))
report_best("KNN (scaled)", gs_knn, X_test, y_test)

# ============ Decision Tree (balanced) ============
# ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πÄ‡∏Å‡∏• ‡πÅ‡∏•‡∏∞‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ class_weight='balanced'
pipe_dt_bal = Pipeline([
    ("clf", DecisionTreeClassifier(class_weight="balanced", random_state=SEED))
])

param_dt_bal = {
    "clf__criterion": ["gini", "entropy"],
    "clf__max_depth": [None, 5, 10, 20],
    "clf__min_samples_split": [2, 5, 10]
}

gs_dt_bal = GridSearchCV(
    pipe_dt_bal, param_dt_bal, cv=cv, scoring=scoring, refit=REFIT, n_jobs=-1, verbose=1
)
t0=time.time(); gs_dt_bal.fit(X_train, y_train); t1=time.time()
print("DT(balanced) grid time: %.2fs" % (t1-t0))
report_best("Decision Tree (balanced)", gs_dt_bal, X_test, y_test)

"""2.2: ‡πÅ‡∏ö‡∏ö‡πÉ‡∏ä‡πâ SMOTE ‡πÉ‡∏ô Pipeline (‡∏ó‡∏≥ oversampling ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô CV)

‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏à‡∏≤‡∏Å data leakage ‡πÅ‡∏•‡∏∞‡∏°‡∏±‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏° Recall ‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ LogReg / SVM
"""

# ‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô: ‡πÉ‡∏ä‡πâ ImbPipeline (‡∏à‡∏≤‡∏Å imblearn) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ SMOTE ‡∏ó‡∏≥ "‡πÉ‡∏ô CV" ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
# ‡πÅ‡∏•‡∏∞‡∏™‡πÄ‡∏Å‡∏•‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏≥ SMOTE (best practice)

# ============ Logistic Regression + SMOTE ============
pipe_lr_sm = ImbPipeline([
    ("smote", SMOTE(random_state=SEED)),
    ("scaler", StandardScaler()),
    ("clf", LogisticRegression(max_iter=2000, random_state=SEED))
])

param_lr_sm = {
    "clf__C": [0.5, 1, 5],
    "clf__solver": ["liblinear", "saga"],
    "clf__class_weight": [None]  # ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ SMOTE ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á balanced ‡∏≠‡∏µ‡∏Å‡∏ä‡∏±‡πâ‡∏ô
}

gs_lr_sm = GridSearchCV(
    pipe_lr_sm, param_lr_sm, cv=cv, scoring=scoring, refit=REFIT, n_jobs=-1, verbose=1
)
t0=time.time(); gs_lr_sm.fit(X_train, y_train); t1=time.time()
print("LR(SMOTE) grid time: %.2fs" % (t1-t0))
report_best("LogReg (+SMOTE + scaling)", gs_lr_sm, X_test, y_test)

# ============ SVM (RBF) + SMOTE ============
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.model_selection import RandomizedSearchCV, StratifiedShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from imblearn.over_sampling import SMOTE
from scipy.stats import loguniform
import numpy as np, time

SEED = 17
pipe_svm_sm = ImbPipeline([
    ("smote", SMOTE(random_state=SEED, k_neighbors=5)),
    ("scaler", StandardScaler()),
    ("clf", SVC(kernel="rbf", class_weight=None, cache_size=2000, random_state=SEED))
])

# ‡∏Ñ‡πâ‡∏ô‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏° 20 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÅ‡∏ó‡∏ô grid ‡πÄ‡∏ï‡πá‡∏°
param_distrib = {
    "clf__C": loguniform(1e-1, 1e+1),        # ~ 0.1 - 10
    "clf__gamma": loguniform(1e-3, 1e-1)     # ~ 0.001 - 0.1
}

# ‡πÉ‡∏ä‡πâ subset 30% ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏≠‡∏ô search ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡∏°‡∏≤‡∏Å ‡πÜ
sss = StratifiedShuffleSplit(n_splits=1, train_size=0.30, random_state=SEED)
idx_sub, _ = next(sss.split(X_train, y_train))
X_sub, y_sub = X_train.iloc[idx_sub], y_train.iloc[idx_sub]

cv = 3
scoring = {"recall": "recall", "f1": "f1", "accuracy": "accuracy"}
REFIT = "recall"

rs = RandomizedSearchCV(
    pipe_svm_sm, param_distributions=param_distrib, n_iter=20,
    cv=cv, scoring=scoring, refit=REFIT, n_jobs=1, random_state=SEED, verbose=1
)

t0 = time.time(); rs.fit(X_sub, y_sub); t1 = time.time()
print(f"SVM(SMOTE) randomized search time: {(t1-t0):.2f}s")
print("Best params:", rs.best_params_, "CV best recall:", rs.best_score_)

# refit ‡∏ö‡∏ô train ‡πÄ‡∏ï‡πá‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
best = rs.best_estimator_
best.fit(X_train, y_train)
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score
y_pred = best.predict(X_test)
print("Test Acc:", accuracy_score(y_test, y_pred))
print("Test Prec:", precision_score(y_test, y_pred))
print("Test Recall:", recall_score(y_test, y_pred))
print(classification_report(y_test, y_pred, digits=4))

# ============ KNN + SMOTE ============
pipe_knn_sm = ImbPipeline([
    ("smote", SMOTE(random_state=SEED)),
    ("scaler", StandardScaler()),
    ("clf", KNeighborsClassifier())
])

param_knn_sm = {
    "clf__n_neighbors": [3, 5, 7, 11],
    "clf__weights": ["uniform", "distance"],
    "clf__metric": ["euclidean", "manhattan"]
}

gs_knn_sm = GridSearchCV(
    pipe_knn_sm, param_knn_sm, cv=cv, scoring=scoring, refit=REFIT, n_jobs=-1, verbose=1
)
t0=time.time(); gs_knn_sm.fit(X_train, y_train); t1=time.time()
print("KNN(SMOTE) grid time: %.2fs" % (t1-t0))
report_best("KNN (+SMOTE + scaling)", gs_knn_sm, X_test, y_test)

# ============ Decision Tree + SMOTE ============
pipe_dt_sm = ImbPipeline([
    ("smote", SMOTE(random_state=SEED)),
    ("clf", DecisionTreeClassifier(random_state=SEED))
])

param_dt_sm = {
    "clf__criterion": ["gini", "entropy"],
    "clf__max_depth": [None, 5, 10, 20],
    "clf__min_samples_split": [2, 5, 10]
}

gs_dt_sm = GridSearchCV(
    pipe_dt_sm, param_dt_sm, cv=cv, scoring=scoring, refit=REFIT, n_jobs=-1, verbose=1
)
t0=time.time(); gs_dt_sm.fit(X_train, y_train); t1=time.time()
print("DT(SMOTE) grid time: %.2fs" % (t1-t0))
report_best("Decision Tree (+SMOTE)", gs_dt_sm, X_test, y_test)

"""<h1>‚öôÔ∏èHyperparameter Tuning ‚Äî ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô & ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•</h1>

<h2>üéØ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥ (‡∏™‡∏£‡∏∏‡∏õ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö)</h2>

<ol>
  <li>
    <b>‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å (Metric Objective)</b><br>
    ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÇ‡∏à‡∏ó‡∏¢‡πå‡πÄ‡∏õ‡πá‡∏ô <b>Class Imbalance</b> ‚Äî ‡πÄ‡∏Ñ‡∏™ ‚Äú‡πÄ‡∏Ñ‡∏•‡∏°‚Äù ‡∏°‡∏µ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ ‚Äú‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏•‡∏°‚Äù ‡∏°‡∏≤‡∏Å<br>
    ‡∏à‡∏∂‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£ <b>‡∏à‡∏±‡∏ö‡πÄ‡∏Ñ‡∏™‡πÄ‡∏Ñ‡∏•‡∏°‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î</b> ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Recall ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏´‡∏•‡∏±‡∏Å<br><br>
    <code>scoring = {"recall":"recall","f1":"f1","accuracy":"accuracy"}<br>
    refit = "recall"</code>
  </li>

  <li>
    <b>‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Data Leakage ‡∏î‡πâ‡∏ß‡∏¢ Pipeline</b><br>
    ‚Ä¢ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏ß‡∏ï‡πà‡∏≠‡∏™‡πÄ‡∏Å‡∏• (LogReg / SVM / KNN) ‚Üí ‡πÉ‡∏ä‡πâ <b>StandardScaler</b> ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô Pipeline ‡πÄ‡∏™‡∏°‡∏≠<br>
    ‚Ä¢ ‡∏Å‡∏£‡∏ì‡∏µ‡πÉ‡∏ä‡πâ SMOTE ‚Üí ‡πÉ‡∏ä‡πâ <b>imblearn.Pipeline</b> ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ Oversampling ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô Train/CV ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
  </li>

  <li>
    <b>‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Class Imbalance (2 ‡πÅ‡∏ó‡∏£‡πá‡∏Å)</b>
    <ul>
      <li>üí° <b>‡πÅ‡∏ó‡∏£‡πá‡∏Å A ‚Äî Balanced Weight:</b> ‡πÉ‡∏ä‡πâ <code>class_weight="balanced"</code> (‡πÑ‡∏°‡πà‡∏ó‡∏≥ SMOTE)</li>
      <li>üî• <b>‡πÅ‡∏ó‡∏£‡πá‡∏Å B ‚Äî SMOTE Oversampling:</b> ‡πÉ‡∏™‡πà <code>SMOTE()</code> ‡πÉ‡∏ô Pipeline (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Train/CV)</li>
    </ul>
  </li>

  <li>
    <b>‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Å‡∏≤‡∏£‡∏à‡∏π‡∏ô‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå (Tuning Range)</b><br>
    <table>
      <thead>
        <tr><th>Model</th><th>Hyperparameters ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏à‡∏π‡∏ô</th></tr>
      </thead>
      <tbody>
        <tr><td><b>Logistic Regression</b></td><td>C ‚àà {0.1, 0.5, 1}, penalty='l2', solver‚àà{'liblinear','lbfgs'}</td></tr>
        <tr><td><b>SVM (RBF)</b></td><td>C ‚àà {0.5,1,5}, gamma ‚àà {'scale', 0.01, 0.1} (+RandomizedSearchCV: C‚àà[0.1,10], gamma‚àà[1e-3,1e-1])</td></tr>
        <tr><td><b>KNN</b></td><td>n_neighbors ‚àà {3,5,7,11}, weights‚àà{'uniform','distance'}, metric‚àà{'euclidean','manhattan'}</td></tr>
        <tr><td><b>Decision Tree</b></td><td>criterion‚àà{'gini','entropy'}, max_depth‚àà{3,5,None}, min_samples_split‚àà{2,5,10}, class_weight='balanced'</td></tr>
      </tbody>
    </table>
  </li>

  <li>
    <b>‡∏•‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏±‡∏ô‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°</b><br>
    ‚Ä¢ ‡πÉ‡∏ä‡πâ <b>RandomizedSearchCV</b> ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ä‡πâ‡∏≤ (‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ SVM + SMOTE) ‡πÅ‡∏ó‡∏ô Grid ‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö<br>
    ‚Ä¢ ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏Å ‚Üí ‡πÉ‡∏ä‡πâ <b>train subset (‚âà30%)</b> ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏≠‡∏ô‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢ Refit ‡∏ö‡∏ô Train ‡πÄ‡∏ï‡πá‡∏°
  </li>

  <li>
    <b>‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Baseline ‡∏Ç‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô</b><br>
    ‚Ä¢ ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô <b>Accuracy / Precision (class=1) / Recall (class=1)</b><br>
    ‚Ä¢ ‡∏£‡∏∞‡∏ö‡∏∏‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•<br>
    ‚Ä¢ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤‡πÅ‡∏ó‡∏£‡πá‡∏Å‡πÉ‡∏î <b>‡πÄ‡∏û‡∏¥‡πà‡∏° Recall</b> ‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å ‡πÅ‡∏•‡∏∞‡πÅ‡∏ó‡∏£‡πá‡∏Å‡πÉ‡∏î <b>‡∏ö‡∏≤‡∏•‡∏≤‡∏ô‡∏ã‡πå Accuracy ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤</b>
  </li>
</ol>

<hr>

<h2>üìä ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏π‡∏ô (‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Baseline ‡∏Ç‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô)</h2>

<table>
<thead>
<tr>
  <th>Group</th>
  <th>Model</th>
  <th>Technique / Tuning</th>
  <th>Accuracy</th>
  <th>Precision (1)</th>
  <th>Recall (1)</th>
  <th>Notes</th>
</tr>
</thead>
<tbody>
<tr><td>Baseline</td><td>Logistic Regression</td><td>no scaler</td><td>0.5659</td><td>0.0857</td><td>0.5801</td><td>‡∏Ç‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô</td></tr>
<tr><td>Tuned (A)</td><td>Logistic Regression</td><td>balanced + scaling (C=0.1, solver=liblinear)</td><td>0.5638</td><td>0.0884</td><td>0.6052</td><td>Recall ‚Üë ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢</td></tr>
<tr><td>Tuned (B)</td><td>Logistic Regression + SMOTE</td><td>SMOTE + scaling (C=0.5, solver=liblinear)</td><td>0.5660</td><td>0.0858</td><td>0.5801</td><td>‡πÉ‡∏Å‡∏•‡πâ baseline</td></tr>

<tr><td>Baseline</td><td>SVM (RBF)</td><td>default, no SMOTE</td><td>0.4944</td><td>0.090</td><td>0.690</td><td>‡∏Ç‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô</td></tr>
<tr><td>Tuned (A)</td><td>SVM (RBF)</td><td>balanced + scaling (C=5, Œ≥=0.01)</td><td>0.4791</td><td>0.0869</td><td><b>0.7290</b></td><td>Recall ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏™‡∏≤‡∏¢ SVM</td></tr>
<tr><td>Tuned (B)</td><td>SVM (RBF) + SMOTE</td><td>SMOTE + scaling (C‚âà0.20, Œ≥‚âà0.0013)</td><td>0.5419</td><td>0.0875</td><td>0.633</td><td>Accuracy ‚Üë ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö baseline</td></tr>

<tr><td>Baseline</td><td>Decision Tree</td><td>gini, max_depth=None</td><td>0.8451</td><td>0.0891</td><td>0.1472</td><td>‡∏Ç‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô</td></tr>
<tr><td>Tuned (A)</td><td>Decision Tree</td><td>balanced (entropy, max_depth=5)</td><td>0.5522</td><td>0.0942</td><td>0.6753</td><td>Recall ‚Üë ‡∏°‡∏≤‡∏Å, Accuracy ‚Üì</td></tr>
<tr><td>Tuned (B)</td><td>Decision Tree + SMOTE</td><td>SMOTE (entropy, max_depth=5)</td><td>0.3618</td><td>0.0756</td><td>0.7766</td><td>Recall ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î, Accuracy ‡∏•‡∏î‡∏´‡∏ô‡∏±‡∏Å</td></tr>

<tr><td>Baseline</td><td>KNN</td><td>k=5, no scaling</td><td>0.7044</td><td>0.0763</td><td>0.3152</td><td>‡∏Ç‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô</td></tr>
<tr><td>Tuned (A)</td><td>KNN (scaled)</td><td>scaler + (k=3, weights=distance, metric=manhattan)</td><td>0.9106</td><td>0.0920</td><td>0.0407</td><td>Acc ‚Üë ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å, Recall ‡∏ï‡πà‡∏≥</td></tr>
<tr><td>Tuned (B)</td><td>KNN + SMOTE</td><td>SMOTE + scaling (k=11, uniform, euclidean)</td><td>0.6575</td><td>0.0788</td><td>0.3939</td><td>Recall ‚Üë ‡πÅ‡∏ï‡πà‡πÅ‡∏•‡∏Å‡∏Å‡∏±‡∏ö Accuracy</td></tr>
</tbody>
</table>

<hr>

<h3>üí¨ ‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°</h3>
<ul>
  <li>‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà <b>‡∏î‡∏±‡∏ô Recall ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î</b>: <b>SVM (RBF, balanced)</b> ‚Üí Recall ‚âà 0.73</li>
  <li>‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà <b>Accuracy ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ baseline</b>: <b>SVM (RBF + SMOTE)</b> ‚Üí Acc ‚âà 0.54</li>
  <li>Decision Tree + SMOTE ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ Recall ‡∏û‡∏∏‡πà‡∏á‡∏ñ‡∏∂‡∏á ‚âà0.78 ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏Å‡∏Å‡∏±‡∏ö Accuracy ‡∏ó‡∏µ‡πà‡∏•‡∏î‡∏•‡∏á</li>
  <li>KNN ‡∏´‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏™‡πÄ‡∏Å‡∏•‡∏°‡∏µ Accuracy ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å‡πÅ‡∏ï‡πà Recall ‡∏ï‡πà‡∏≥ ‚Üí ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ö‡∏≤‡∏•‡∏≤‡∏ô‡∏ã‡πå‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤</li>
</ul>

<p><b>‡∏™‡∏£‡∏∏‡∏õ:</b> ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ <code>class_weight="balanced"</code> ‡πÅ‡∏•‡∏∞ <code>SMOTE+Pipeline</code> ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏Ñ‡∏•‡∏°‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß baseline ‡∏Ç‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô</p>

<h3>3. Threshold &amp; Calibration</h3>
<ul>
  <li>‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö <b>threshold</b> ‡∏Ç‡∏≠‡∏á class 1 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤ <b>Recall</b></li>
  <li>‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ <b>CalibratedClassifierCV</b> (‡∏Å‡∏£‡∏ì‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ probability ‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£)</li>
</ul>

<h2>üéØ Threshold &amp; Calibration</h2>

<p><b>‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ:</b> <u>Logistic Regression (balanced + scaling)</u> ‡∏ó‡∏µ‡πà‡∏à‡∏π‡∏ô‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏•‡πâ‡∏ß<br>
‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: Logistic Regression ‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤ <code>predict_proba</code> ‡∏ó‡∏µ‡πà‡∏™‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á
‡∏à‡∏∂‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö threshold ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡πà‡∏≤ Recall ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û</p>


<h3>üß© ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£</h3>
<ol>
  <li>‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• <code>LogReg (balanced)</code> ‡∏ó‡∏µ‡πà‡∏à‡∏π‡∏ô‡πÅ‡∏•‡πâ‡∏ß (<code>gs_lr_bal.best_estimator_</code>) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡πà‡∏≤ <code>predict_proba</code> ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö class 1</li>
  <li>‡∏™‡πÅ‡∏Å‡∏ô‡∏Ñ‡πà‡∏≤ threshold ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà 0.00‚Äì1.00 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤ <b>Recall (class 1)</b> ‡∏ï‡∏≤‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (~0.60‚Äì0.70)</li>
  <li>‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏Å‡πà‡∏≠‡∏ô/‡∏´‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏î‡πâ‡∏ß‡∏¢ <b>Accuracy</b>, <b>Precision(1)</b>, <b>Recall(1)</b>, <b>F1(1)</b> ‡πÅ‡∏•‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö</li>
</ol>

<h3>‚öñÔ∏è ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à</h3>
<ul>
  <li><b>‡∏´‡∏≤‡∏Å‡πÄ‡∏ô‡πâ‡∏ô ‚Äú‡∏à‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‚Äù (Recall ‡∏™‡∏π‡∏á):</b> ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å threshold ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ Recall ~0.65‚Äì0.70 ‡πÅ‡∏°‡πâ Accuracy ‡∏à‡∏∞‡∏•‡∏î‡∏•‡∏á</li>
  <li><b>‡∏´‡∏≤‡∏Å‡πÄ‡∏ô‡πâ‡∏ô ‚Äú‡∏™‡∏°‡∏î‡∏∏‡∏•‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‚Äù:</b> ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å threshold ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤ <b>F1(1)</b> ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (‡∏à‡∏∏‡∏î‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Precision‚ÄìRecall)</li>
</ul>
"""

# ==== Imports (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ import) ====
import numpy as np
from sklearn.metrics import (
    classification_report, accuracy_score, precision_score, recall_score,
    precision_recall_curve
)

# ==== Helper: ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏ó‡∏µ‡πà threshold ‡πÉ‡∏î ‡πÜ ====
def eval_at_threshold(y_true, y_prob, thr, title=""):
    y_pred = (y_prob >= thr).astype(int)
    acc = accuracy_score(y_true, y_pred)
    pre = precision_score(y_true, y_pred, zero_division=0)
    rec = recall_score(y_true, y_pred)
    print(f"\n--- {title} (thr={thr:.2f}) ---")
    print(f"Accuracy : {acc:.4f}")
    print(f"Precision: {pre:.4f}")
    print(f"Recall   : {rec:.4f}")
    print(classification_report(y_true, y_pred, digits=4))
    return acc, pre, rec

# ==== Helper: ‡∏´‡∏≤ threshold ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ Recall >= ‡πÄ‡∏õ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ====
def threshold_for_target_recall(y_true, y_prob, target_recall=0.70):
    prec, rec, thr = precision_recall_curve(y_true, y_prob)
    # ‡∏´‡∏≤ index ‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà Recall ‡∏ñ‡∏∂‡∏á‡πÄ‡∏õ‡πâ‡∏≤
    idx = np.where(rec >= target_recall)[0]
    if len(idx) == 0:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏õ‡πÑ‡∏°‡πà‡∏ñ‡∏∂‡∏á‡πÄ‡∏õ‡πâ‡∏≤ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ 0.50 ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤ fallback
        return 0.50
    # thresholds ‡∏¢‡∏≤‡∏ß‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ rec ‡∏≠‡∏¢‡∏π‡πà 1 ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
    pick = min(idx[0], len(thr) - 1)
    return float(thr[pick])

# ==== Helper: ‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏•‡∏≤‡∏¢ threshold ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Å‡∏∞‡∏ó‡∏±‡∏î‡∏£‡∏±‡∏î ====
def sweep_thresholds(y_true, y_prob, thresholds, title="Model"):
    rows = []
    for thr in thresholds:
        y_pred = (y_prob >= thr).astype(int)
        rows.append([
            thr,
            accuracy_score(y_true, y_pred),
            precision_score(y_true, y_pred, zero_division=0),
            recall_score(y_true, y_pred),
        ])
    import pandas as pd
    df = pd.DataFrame(rows, columns=["threshold","accuracy","precision_1","recall_1"])
    print(f"\n=== Threshold sweep: {title} ===")
    display(df)
    return df

# ===== Logistic Regression (balanced) ‚Äî Threshold tuning =====
# ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏à‡∏π‡∏ô‡πÅ‡∏•‡πâ‡∏ß
model_lr = gs_lr_bal.best_estimator_

# 1) ‡∏î‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô class=1
y_prob_lr = model_lr.predict_proba(X_test)[:, 1]

# 2) ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ó‡∏µ‡πà threshold ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô 0.50 ‡πÅ‡∏•‡∏∞‡∏ö‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡∏•‡∏≠‡∏á
eval_at_threshold(y_test, y_prob_lr, 0.50, title="LogReg (balanced)")
for thr in [0.40, 0.35, 0.30, 0.25]:
    eval_at_threshold(y_test, y_prob_lr, thr, title="LogReg (balanced)")

# 3) ‡∏´‡∏≤ threshold ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Recall ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô ~0.70)
thr70_lr = threshold_for_target_recall(y_test, y_prob_lr, target_recall=0.70)
eval_at_threshold(y_test, y_prob_lr, thr70_lr, title="LogReg (balanced) @ target recall‚âà0.70")

# 4) ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏•‡∏≤‡∏¢ threshold
_ = sweep_thresholds(y_test, y_prob_lr, [0.50, 0.45, 0.40, 0.35, 0.30], title="LogReg (balanced)")
print("Chosen threshold for ~0.70 recall (LogReg):", round(thr70_lr, 3))

from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt

y_prob = gs_lr_bal.best_estimator_.predict_proba(X_test)[:, 1]
prec, rec, thr = precision_recall_curve(y_test, y_prob)

plt.plot(rec, prec)
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision‚ÄìRecall Curve (LogReg balanced)")
plt.axvline(0.7, color="r", linestyle="--", label="Target Recall ‚âà 0.7")
plt.legend()
plt.show()

# === Evaluate Logistic Regression at threshold = 0.162 ===
import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix
)

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏π‡∏ô‡πÅ‡∏•‡πâ‡∏ß (‡πÅ‡∏Å‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏≠‡∏∑‡πà‡∏ô)
model = gs_lr_bal.best_estimator_   # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ gs_lr_bal ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ log_reg_balanced ‡∏ó‡∏µ‡πà‡∏à‡∏π‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à

# 1) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì probability class 1 ‡∏ö‡∏ô test set
y_prob = model.predict_proba(X_test)[:, 1]

# 2) ‡∏Å‡∏≥‡∏´‡∏ô‡∏î threshold ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
thr = 0.162

# 3) ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à class ‡∏î‡πâ‡∏ß‡∏¢ threshold ‡πÉ‡∏´‡∏°‡πà
y_pred_thr = (y_prob >= thr).astype(int)

# 4) ‡∏™‡∏£‡∏∏‡∏õ‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î
acc = accuracy_score(y_test, y_pred_thr)
prec = precision_score(y_test, y_pred_thr, zero_division=0)
rec  = recall_score(y_test, y_pred_thr, zero_division=0)
f1   = f1_score(y_test, y_pred_thr, zero_division=0)

print(f"--- Logistic Regression (balanced) @ threshold = {thr:.3f} ---")
print(f"Accuracy : {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall   : {rec:.4f}")
print(f"F1-score : {f1:.4f}\n")

print(classification_report(y_test, y_pred_thr, digits=4))

cm = confusion_matrix(y_test, y_pred_thr)
cm_df = pd.DataFrame(cm, index=["Actual 0","Actual 1"], columns=["Pred 0","Pred 1"])
print("Confusion Matrix:\n", cm_df)

# (‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö) ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏•‡πá‡∏Å‡πÜ ‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏ß‡πâ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏≤‡πÑ‡∏õ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö threshold 0.50
summary = pd.DataFrame([
    {"Model":"LogReg (balanced)", "Threshold":0.50, "Accuracy":0.5634, "Precision(1)":0.0877, "Recall(1)":0.6000},
    {"Model":"LogReg (balanced)", "Threshold":thr,  "Accuracy":acc,     "Precision(1)":prec,    "Recall(1)":rec}
])
summary

"""<h3>üéØ Threshold Comparison (Before vs After)</h3>

<table>
  <tr>
    <th>Model</th>
    <th>Threshold</th>
    <th>Accuracy</th>
    <th>Precision (class 1)</th>
    <th>Recall (class 1)</th>
    <th>‡∏™‡∏£‡∏∏‡∏õ</th>
  </tr>
  <tr>
    <td>Logistic Regression (balanced)</td>
    <td>0.50 (default)</td>
    <td>0.563</td>
    <td>0.0877</td>
    <td>0.600</td>
    <td>Baseline ‚Äì ‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏î‡∏µ‡πÅ‡∏ï‡πà‡∏à‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏°‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö</td>
  </tr>
  <tr>
    <td>Logistic Regression (balanced)</td>
    <td><b>0.162</b> (tuned)</td>
    <td>0.066</td>
    <td>0.0657</td>
    <td><b>1.000</b></td>
    <td>Recall ‡∏î‡∏µ‡∏™‡∏∏‡∏î ‡∏à‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏°‡∏Ñ‡∏£‡∏ö‡πÅ‡∏ï‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á</td>
  </tr>
</table>

<p><b>‡∏™‡∏£‡∏∏‡∏õ:</b> ‡∏Å‡∏≤‡∏£‡∏•‡∏î threshold ‡∏à‡∏≤‡∏Å 0.50 ‚Üí 0.162 ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏° Recall ‡∏à‡∏≤‡∏Å ~0.60 ‡πÄ‡∏õ‡πá‡∏ô 1.00
‡πÅ‡∏ï‡πà‡∏•‡∏î Accuracy ‡∏•‡∏á‡∏°‡∏≤‡∏Å ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏µ‡πà ‚Äú‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏ï‡∏£‡∏ß‡∏à‡∏ã‡πâ‡∏≥‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á‚Äù</p>

<h3>üìà Threshold Optimization Summary</h3>
<p>
‡∏´‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏±‡∏ö <code>threshold</code> ‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô 0.50 ‚Üí 0.162 ‡∏û‡∏ö‡∏ß‡πà‡∏≤:
</p>
<ul>
  <li>Recall ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å ‚âà 0.60 ‚Üí ‚âà 1.00 (‡∏à‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏Å‡∏∑‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)</li>
  <li>Accuracy ‡∏•‡∏î‡∏à‡∏≤‡∏Å ‚âà 0.56 ‚Üí ‚âà 0.07 ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ 1 ‡πÄ‡∏Å‡∏∑‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î</li>
  <li>‡πÅ‡∏™‡∏î‡∏á trade-off ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô (<b>Recall</b>) ‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (<b>Precision</b>)</li>
</ul>
<p>
‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å threshold ‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à:<br>
<b>‡πÄ‡∏ô‡πâ‡∏ô‡∏à‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏°</b> ‚Üí threshold ‚âà 0.2 <br>
<b>‡πÄ‡∏ô‡πâ‡∏ô‡∏™‡∏°‡∏î‡∏∏‡∏•</b> ‚Üí threshold ‚âà 0.4‚Äì0.5
</p>

<hr>

<br>

<h1>üß© 4. Ensemble & Deep Models (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢)</h1>

<p>‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏±‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå (Hyperparameter Tuning) ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏∏‡∏î‡∏ï‡∏±‡∏î (Threshold Calibration)
‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏ß ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÄ‡∏ô‡πâ‡∏ô <b>‚Äú‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏ß‡∏° (Ensemble)‚Äù</b>
‡∏´‡∏£‡∏∑‡∏≠ <b>‚Äú‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å (Deep Model)‚Äù</b> ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏≠‡∏µ‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà</p>

<hr>

<h2>üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Section ‡∏ô‡∏µ‡πâ</h2>
<ul>
  <li>‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•</li>
  <li>‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ß‡πà‡∏≤ Deep Model ‡πÉ‡∏´‡πâ Recall ‡∏´‡∏£‡∏∑‡∏≠ F1-score ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏à‡∏≤‡∏Å baseline ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà</li>
  <li>‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î data leakage ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô preprocessing ‡πÄ‡∏î‡∏¥‡∏°</li>
</ul>

<hr>

üå≤ 4.1 Random Forest (Bagging Ensemble)

<p>‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡πâ‡∏ô (Decision Trees) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Ç‡∏∂‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏•‡∏î overfitting</p>

<ul>
  <li>‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå <code>class_weight='balanced_subsample'</code> ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡∏î‡πÄ‡∏ä‡∏¢ class imbalance</li>
  <li>‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• tabular ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ</li>
</ul>

<p><b>‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á:</b> Accuracy ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ Logistic Regression, Recall ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á,
‡πÄ‡∏õ‡πá‡∏ô baseline ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö tabular data</p>
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
import time

start = time.time()
rf = RandomForestClassifier(
    n_estimators=200,
    max_depth=5,
    min_samples_split=2,
    criterion='entropy',
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)

print("--- Random Forest ---")
print("Accuracy:", accuracy_score(y_test, rf_pred))
print(classification_report(y_test, rf_pred))
print("Training time:", time.time() - start, "seconds")

"""ü§ñ 4.2 MLP Neural Network (Deep Learning)

<p>‡πÇ‡∏°‡πÄ‡∏î‡∏• Neural Network ‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (Multi-layer Perceptron)
‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ traditional ML ‡πÄ‡∏î‡∏¥‡∏°</p>

<ul>
  <li>‡πÉ‡∏ä‡πâ hidden layers 2 ‡∏ä‡∏±‡πâ‡∏ô (64 ‚Üí 32 neurons)</li>
  <li>Activation function: <code>relu</code></li>
  <li>Optimizer: <code>adam</code></li>
</ul>

<p><b>‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á:</b> ‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏°‡∏≤‡∏Å ‡∏≠‡∏≤‡∏à‡πÑ‡∏î‡πâ‡∏ú‡∏•‡πÉ‡∏Å‡∏•‡πâ Logistic Regression
‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏ó‡∏î‡∏•‡∏≠‡∏á ‚ÄúDifferent model‚Äù ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏Å‡∏≥‡∏´‡∏ô‡∏î</p>
"""

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, accuracy_score
import time

start = time.time()

mlp = MLPClassifier(
    hidden_layer_sizes=(64, 32),
    activation='relu',
    solver='adam',
    max_iter=200,
    random_state=42
    # Removed class_weight='balanced' as it's not supported
)

# Train on the resampled data
mlp.fit(X_train_resampled, y_train_resampled)

mlp_pred = mlp.predict(X_test)

print("--- Neural Network (MLP) trained on SMOTE data ---")
print("Accuracy:", accuracy_score(y_test, mlp_pred))
print(classification_report(y_test, mlp_pred))
print("Training time:", time.time() - start, "seconds")

"""4.3 üöÄ XGBoost (Gradient Boosted Trees)

<p><b>‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î:</b>
XGBoost ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏™‡∏£‡∏¥‡∏°‡πÅ‡∏£‡∏á‡πÅ‡∏ö‡∏ö‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ (Gradient Boosting Trees)
‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏ö‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡∏±‡πâ‡∏ô ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡πÅ‡∏Å‡πâ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô (non-linear) ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô one-hot ‡πÅ‡∏•‡πâ‡∏ß</p>

<ul>
  <li><b>‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å:</b> ‡∏ó‡∏ô‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏™‡πÄ‡∏Å‡∏•‡∏Ç‡∏≠‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå, ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£ overfitting ‡πÑ‡∏î‡πâ‡∏î‡∏µ, ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà</li>
  <li><b>‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏°‡∏∑‡∏≠ class imbalance:</b> ‡πÉ‡∏ä‡πâ <code>scale_pos_weight = balance_ratio</code>
      ‡∏ã‡∏∂‡πà‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô <i>negative / positive</i> ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÑ‡∏î‡πâ‡∏ô‡πâ‡∏≠‡∏¢</li>
  <li><b>‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:</b>
      <ul>
        <li><code>n_estimators=300</code></li>
        <li><code>max_depth=5</code></li>
        <li><code>learning_rate=0.1</code></li>
        <li><code>subsample=0.8</code>, <code>colsample_bytree=0.8</code></li>
        <li><code>scale_pos_weight=balance_ratio</code>, <code>eval_metric='logloss'</code></li>
      </ul>
  </li>

  <li><b>‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏à‡∏π‡∏ô‡∏ï‡πà‡∏≠:</b>
    ‡∏•‡∏î <code>learning_rate</code> ‚Üí ‡πÄ‡∏û‡∏¥‡πà‡∏° <code>n_estimators</code>,
    ‡∏õ‡∏£‡∏±‡∏ö <code>max_depth</code> / <code>min_child_weight</code> ‡πÅ‡∏•‡∏∞ <code>gamma</code> ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î overfit
  </li>
</ul>

<p><b>‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏:</b>
‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏ô‡πâ‡∏ô Recall ‡∏™‡∏π‡∏á (‡∏à‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏¢‡∏≠‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô)
‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤ <code>scale_pos_weight</code> ‡∏ó‡∏µ‡∏•‡∏∞‡∏ô‡πâ‡∏≠‡∏¢ ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏ö‡∏ô‡∏ä‡∏∏‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö</p>
"""

import xgboost as xgb
start = time.time()

xgb_clf = xgb.XGBClassifier(
    n_estimators=300,
    max_depth=5,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=balance_ratio,  # ‡πÉ‡∏ä‡πâ‡∏ñ‡πâ‡∏≤ class ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏î‡∏∏‡∏•
    random_state=42,
    eval_metric='logloss'
)

xgb_clf.fit(X_train, y_train)
xgb_pred = xgb_clf.predict(X_test)

print("--- XGBoost ---")
print("Accuracy:", accuracy_score(y_test, xgb_pred))
print(classification_report(y_test, xgb_pred))
print("Training time:", time.time() - start, "seconds")

# Calculate the balance ratio for scale_pos_weight in XGBoost
# This is typically calculated as the number of negative samples / number of positive samples
balance_ratio = (y_train == 0).sum() / (y_train == 1).sum()
print(f"Calculated balance_ratio: {balance_ratio:.2f}")

"""4.4 ‚ö° LightGBM (Leaf-wise Gradient Boosting)

<p><b>‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î:</b>
LightGBM ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Boosting ‡πÅ‡∏ö‡∏ö Leaf-wise ‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡πÉ‡∏ö (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ä‡∏±‡πâ‡∏ô)
‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Å‡∏ß‡πà‡∏≤ XGBoost ‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏Å‡∏£‡∏ì‡∏µ
‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏°‡∏≤‡∏Å</p>

<ul>
  <li><b>‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å:</b> ‡∏£‡∏±‡∏ô‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ XGBoost, ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö <code>class_weight='balanced'</code>
      ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ class imbalance ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥</li>
  <li><b>‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:</b>
      <ul>
        <li><code>n_estimators=300</code>, <code>learning_rate=0.05</code></li>
        <li><code>max_depth=-1</code> (‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å)</li>
        <li><code>subsample=0.8</code>, <code>colsample_bytree=0.8</code></li>
        <li><code>class_weight='balanced'</code>, <code>random_state=42</code></li>
      </ul>
  </li>

  <li><b>‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏à‡∏π‡∏ô‡∏ï‡πà‡∏≠:</b>
      ‡∏õ‡∏£‡∏±‡∏ö <code>num_leaves</code>, <code>min_child_samples</code>,
      <code>feature_fraction</code>, <code>bagging_fraction</code>, ‡πÅ‡∏•‡∏∞ <code>bagging_freq</code>
      ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° overfitting ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û</li>
</ul>

<p><b>‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏:</b>
‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏±‡∏ô Recall ‡πÉ‡∏´‡πâ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ <code>class_weight</code>
‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÉ‡∏ä‡πâ <code>predict_proba</code>
‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô <b>Threshold & Calibration</b> ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô</p>
"""

import lightgbm as lgb
start = time.time()

lgb_clf = lgb.LGBMClassifier(
    n_estimators=300,
    max_depth=-1,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    class_weight='balanced',
    random_state=42
)

lgb_clf.fit(X_train, y_train)
lgb_pred = lgb_clf.predict(X_test)

print("--- LightGBM ---")
print("Accuracy:", accuracy_score(y_test, lgb_pred))
print(classification_report(y_test, lgb_pred))
print("Training time:", time.time() - start, "seconds")

"""üß† 4.5 Voting Ensemble

<p>‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• (Logistic Regression, Decision Tree, SVM)
‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏ß‡∏ï (majority vote ‡∏´‡∏£‡∏∑‡∏≠ average probability)</p>

<ul>
  <li>‡πÉ‡∏ä‡πâ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î <b>‚ÄúWisdom of Models‚Äù</b> ‚Äî ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏µ‡∏°‡∏∏‡∏°‡∏°‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô</li>
  <li>‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ <code>voting='soft'</code> ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏ß‡∏ï‡πÅ‡∏ö‡∏ö‡πÅ‡∏Ç‡πá‡∏á</li>
</ul>

<p><b>‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á:</b> Accuracy ‡πÉ‡∏Å‡∏•‡πâ Logistic Regression ‡πÅ‡∏ï‡πà Recall ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏∑‡πà‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏ö‡πÄ‡∏Ñ‡∏™ minority class ‡πÑ‡∏î‡πâ‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô</p>
"""

from sklearn.ensemble import VotingClassifier
from sklearn.metrics import classification_report, accuracy_score
import numpy as np

# --- ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏π‡∏ô‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß ---
# Ensure these GridSearchCV objects (gs_lr_bal, gs_svm_bal, gs_dt_bal)
# have been run successfully before executing this cell.
log_reg_best = gs_lr_bal.best_estimator_
svm_best = gs_svm_bal.best_estimator_
dt_best = gs_dt_bal.best_estimator_ # Use gs_dt_bal instead of gs_rf_bal

# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Voting Ensemble ---
voting_clf = VotingClassifier(
    estimators=[
        ('logreg', log_reg_best),
        ('svm', svm_best),
        ('dt', dt_best) # Use the Decision Tree estimator
    ],
    voting='hard'   # ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ predict_proba ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô
)

# --- ‡πÄ‡∏ó‡∏£‡∏ô ---
voting_clf.fit(X_train, y_train)

# --- ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ---
y_pred_voting = voting_clf.predict(X_test)

# --- ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• ---
print("=== Voting Ensemble ===")
print("Accuracy :", accuracy_score(y_test, y_pred_voting))
print(classification_report(y_test, y_pred_voting, digits=4))

"""4.6 üß± Stacking Ensemble (Meta-learner = Logistic Regression)

<p><b>‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î:</b> ‡∏£‡∏ß‡∏° ‚Äú‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á‚Äù ‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ê‡∏≤‡∏ô (base learners) ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (<i>meta-learner</i>)
‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ú‡∏™‡∏≤‡∏ô‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô/‡∏™‡∏Å‡∏≠‡∏£‡πå‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÉ‡∏´‡∏°‡πà</p>

<ul>
  <li><b>Base learners:</b> Logistic Regression (‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡πà‡∏≤‡∏¢), SVM (‡∏à‡∏±‡∏ö‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô), Decision Tree (‡∏õ‡∏è‡∏¥‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÑ‡∏°‡πà‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô)</li>
  <li><b>Meta-learner:</b> <code>LogisticRegression(class_weight='balanced')</code> ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏Ñ‡∏•‡∏°</li>
  <li><b>CV:</b> ‡πÉ‡∏ä‡πâ <code>cv=5</code> ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô <code>StackingClassifier</code> ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏£‡∏±‡πà‡∏ß‡πÑ‡∏´‡∏•‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ä‡∏±‡πâ‡∏ô</li>
  <li><b>‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô:</b> Accuracy, Precision/Recall (class 1), F1 (class 1)</li>
</ul>

<p><b>‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:</b>
‡∏ñ‡πâ‡∏≤ base learner ‡∏ö‡∏≤‡∏á‡∏ï‡∏±‡∏ß (‡πÄ‡∏ä‡πà‡∏ô <code>SVC</code>) ‡πÑ‡∏°‡πà‡∏°‡∏µ <code>predict_proba</code>, ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
‡πÉ‡∏ä‡πâ <code>probability=True</code> ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏∏‡πâ‡∏°‡∏î‡πâ‡∏ß‡∏¢ <code>CalibratedClassifierCV</code>
‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Stacking ‡πÉ‡∏ä‡πâ <code>stack_method='predict_proba'</code> ‡πÑ‡∏î‡πâ</p>


"""

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Stacking Ensemble ---
stacking_clf = StackingClassifier(
    estimators=[
        ('logreg', log_reg_best),
        ('svm', svm_best),
        ('dt', dt_best)
    ],
    final_estimator=LogisticRegression(
        max_iter=1000,
        random_state=42,
        class_weight='balanced'
    ),
    cv=5,
    n_jobs=-1
)


# --- ‡πÄ‡∏ó‡∏£‡∏ô ---
stacking_clf.fit(X_train, y_train)

# --- ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ---
y_pred_stack = stacking_clf.predict(X_test)

# --- ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• ---
print("=== Stacking Ensemble ===")
print("Accuracy :", accuracy_score(y_test, y_pred_stack))
print(classification_report(y_test, y_pred_stack, digits=4))

"""4.7 üîÑ Stacking Ensemble ‚Äî ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Meta-model ‡πÄ‡∏õ‡πá‡∏ô RandomForest

<p><b>‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏•‡∏≠‡∏á:</b> ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏™‡∏Å‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á base models ‡∏Å‡∏±‡∏ö target ‡πÄ‡∏õ‡πá‡∏ô non-linear,
‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ meta-model ‡∏ó‡∏µ‡πà‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡∏Å‡∏ß‡πà‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô <code>RandomForestClassifier</code>)
‡∏≠‡∏≤‡∏à‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏≤‡∏£‡∏ú‡∏™‡∏°‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡πÇ‡∏•‡∏à‡∏¥‡∏™‡∏ï‡∏¥‡∏Å ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡∏î‡∏±‡∏ô Recall/F1</p>

<ul>
  <li><b>Meta-learner:</b> <code>RandomForestClassifier(class_weight='balanced')</code></li>
  <li><b>‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:</b> ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πÄ‡∏Å‡∏•‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå, ‡∏à‡∏±‡∏ö non-linearity ‡πÑ‡∏î‡πâ</li>
  <li><b>‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á:</b> ‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á overfitting ‡πÑ‡∏î‡πâ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡πÄ‡∏¢‡∏≠‡∏∞/‡∏•‡∏∂‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‚Üí ‡∏Ñ‡∏∏‡∏°‡∏î‡πâ‡∏ß‡∏¢ <code>n_estimators</code>, <code>max_depth</code>, <code>min_samples_leaf</code></li>
</ul>

<p><b>‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô:</b> ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô meta-LogReg:
‡∏î‡∏π‡∏ß‡πà‡∏≤ Recall (class 1) ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡πÅ‡∏•‡∏∞ F1 ‡∏£‡∏ß‡∏°‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡πÇ‡∏à‡∏ó‡∏¢‡πå class imbalance</p>


"""

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Stacking Ensemble ---
from sklearn.ensemble import RandomForestClassifier

stacking_clf = StackingClassifier(
    estimators=[
        ('logreg', log_reg_best),
        ('svm', svm_best),
        ('dt', dt_best)
    ],
    final_estimator=RandomForestClassifier(
        n_estimators=100,
        random_state=42,
        class_weight='balanced'
    ),
    cv=5,
    n_jobs=-1
)



# --- ‡πÄ‡∏ó‡∏£‡∏ô ---
stacking_clf.fit(X_train, y_train)

# --- ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ---
y_pred_stack = stacking_clf.predict(X_test)

# --- ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• ---
print("=== Stacking Ensemble ===")
print("Accuracy :", accuracy_score(y_test, y_pred_stack))
print(classification_report(y_test, y_pred_stack, digits=4))

"""<h2>5. Project Summary &amp; Takeaways</h2>

<p><b>‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö baseline ‡πÄ‡∏î‡∏¥‡∏°:</b></p>

<table>
  <tr>
    <th>Model</th>
    <th>Technique</th>
    <th>Accuracy</th>
    <th>Precision (1)</th>
    <th>Recall (1)</th>
    <th>‡∏™‡∏£‡∏∏‡∏õ</th>
  </tr>

  <!-- Baseline -->
  <tr>
    <td>Baseline ‚Äì Logistic Regression</td>
    <td>Default (threshold=0.50)</td>
    <td>0.566</td>
    <td>0.0857</td>
    <td>0.580</td>
    <td>‡∏ú‡∏•‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô‡∏à‡∏≤‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô baseline ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á</td>
  </tr>

  <!-- Logistic Regression (tuned) -->
  <tr>
    <td>LogReg (tuned)</td>
    <td>class_weight='balanced' + Scaling</td>
    <td>0.5638</td>
    <td>0.0884</td>
    <td>0.6052</td>
    <td>Recall ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏à‡∏≤‡∏Å baseline</td>
  </tr>

  <!-- Threshold optimization -->
  <tr>
    <td>LogReg (Threshold = 0.40)</td>
    <td>Recall optimization (lower threshold)</td>
    <td>0.2752</td>
    <td>0.0751</td>
    <td><b>0.8866</b></td>
    <td>Recall ‡∏û‡∏∏‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å ‡πÅ‡∏ï‡πà Accuracy ‡∏•‡∏î‡∏•‡∏á‡∏°‡∏≤‡∏Å (trade-off ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô)</td>
  </tr>

  <!-- SVM tuned -->
  <tr>
    <td>SVM (RBF, balanced)</td>
    <td>Tuned (C=5, Œ≥=0.01)</td>
    <td>0.4791</td>
    <td>0.0869</td>
    <td><b>0.7290</b></td>
    <td>‡∏î‡∏±‡∏ô Recall ‡πÑ‡∏î‡πâ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß</td>
  </tr>

  <!-- KNN tuned -->
  <tr>
    <td>KNN (scaled)</td>
    <td>n=3, Manhattan, distance</td>
    <td>0.9106</td>
    <td>0.0920</td>
    <td>0.0407</td>
    <td>Accuracy ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å‡πÅ‡∏ï‡πà Recall ‡∏ï‡πà‡∏≥ (‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö imbalance)</td>
  </tr>

  <!-- Decision Tree -->
  <tr>
    <td>Decision Tree</td>
    <td>Entropy, max_depth=5, balanced</td>
    <td>0.5522</td>
    <td>0.0942</td>
    <td>0.6753</td>
    <td>Recall ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å</td>
  </tr>

  <!-- Voting Ensemble -->
  <tr>
    <td>Voting Ensemble</td>
    <td>Soft Voting (LR + DT + SVM)</td>
    <td>0.5145</td>
    <td>0.0915</td>
    <td>0.7152</td>
    <td>Recall ‡∏™‡∏π‡∏á‡πÉ‡∏Å‡∏•‡πâ SVM ‡πÅ‡∏ï‡πà‡∏ú‡∏•‡∏ô‡∏¥‡πà‡∏á‡∏Å‡∏ß‡πà‡∏≤</td>
  </tr>

  <!-- Stacking LR -->
  <tr>
    <td>Stacking (meta = Logistic Regression)</td>
    <td>LR + DT + SVM ‚Üí meta-LR</td>
    <td>0.5252</td>
    <td>0.0931</td>
    <td>0.7126</td>
    <td>Recall ‡∏î‡∏µ‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Å‡∏ß‡πà‡∏≤ Voting ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢</td>
  </tr>

  <!-- Stacking RF -->
  <tr>
    <td>Stacking (meta = Random Forest)</td>
    <td>LR + DT + SVM ‚Üí meta-RF</td>
    <td>0.9334</td>
    <td>0.2222</td>
    <td>0.0052</td>
    <td>‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏≠‡∏ô‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡∏•‡∏≤‡∏™ 0 (Recall ‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å)</td>
  </tr>
</table>

<h2>Overview</h2>
<p>
‡∏á‡∏≤‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£ <b>‡∏™‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô (Car Insurance Claim)</b> ‡πÇ‡∏î‡∏¢‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô baseline ‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á
‡πÅ‡∏•‡∏∞‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡∏≤‡∏°‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå ‚Äî <b>‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô preprocessing, ‡∏õ‡∏£‡∏±‡∏ö hyperparameters, ‡∏ó‡∏î‡∏•‡∏≠‡∏á threshold &amp; calibration,
‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ Ensemble</b> ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÅ‡∏ö‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (journey-based improvement).
</p>

<h2>Dataset &amp; Target</h2>
<ul>
  <li>Target: <b>is_claim</b> (binary, imbalanced)</li>
  <li>Feature ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏£‡∏ñ, ‡∏≠‡∏≤‡∏¢‡∏∏‡∏£‡∏ñ, ‡∏≠‡∏≤‡∏¢‡∏∏‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå, ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà, ‡∏Ñ‡πà‡∏≤ rating ‡∏ï‡πà‡∏≤‡∏á ‡πÜ</li>
  <li>Metric ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: <b>Recall (class=1)</b> ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏à‡∏±‡∏ö‡πÄ‡∏Ñ‡∏™‡πÄ‡∏Ñ‡∏•‡∏°‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î</li>
</ul>

<h2>What We Reproduced (Baseline)</h2>
<ul>
  <li>‡∏£‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô 5 ‡∏ï‡∏±‡∏ß (Decision Tree, Logistic Regression, SVM, Naive Bayes, KNN)</li>
  <li>‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡πà‡∏≤ baseline (‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ train/test split ‡πÄ‡∏î‡∏¥‡∏°)</li>
</ul>

<h2>What We Improved</h2>
<ol>
  <li><b>Preprocessing ‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å:</b> ‡πÉ‡∏ä‡πâ StandardScaler ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô Pipeline,
      ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö <code>class_weight='balanced'</code> ‡∏Å‡∏±‡∏ö <code>SMOTE</code> ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ imbalance.</li>
  <li><b>Hyperparameter Tuning:</b> Grid + Randomized Search
      ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ <code>scoring={"recall","f1","accuracy"}</code> ‡πÅ‡∏•‡∏∞ <code>refit="recall"</code>.</li>
  <li><b>Threshold &amp; Calibration:</b> ‡πÉ‡∏ä‡πâ Logistic Regression (balanced + scaling) ‡∏ó‡∏µ‡πà‡∏à‡∏π‡∏ô‡πÅ‡∏•‡πâ‡∏ß
      ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å threshold ‡πÉ‡∏´‡∏°‡πà (‚âà0.4) ‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ Recall ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÇ‡∏î‡∏¢‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ trade-off.</li>
  <li><b>Ensemble:</b> ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á <i>Soft Voting</i> ‡πÅ‡∏•‡∏∞ <i>Stacking</i> (meta-LogReg, meta-RF)
      ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏ß‡∏°‡∏à‡∏∏‡∏î‡πÅ‡∏Ç‡πá‡∏á‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô.</li>
</ol>

<h2>Key Results (‡∏™‡∏£‡∏∏‡∏õ‡∏¢‡πà‡∏≠)</h2>
<ul>
  <li><b>LogReg (balanced + scaling, tuned):</b> Recall ‚âà0.60 ‡∏ó‡∏µ‡πà threshold=0.50;
      ‡∏õ‡∏£‡∏±‡∏ö threshold ‚Üí Recall ‡∏™‡∏π‡∏á‡∏ñ‡∏∂‡∏á 0.88 (‡πÅ‡∏•‡∏Å‡∏Å‡∏±‡∏ö Accuracy ‡∏ï‡πà‡∏≥‡∏•‡∏á).</li>
  <li><b>SVM-RBF (balanced, tuned):</b> Recall ‚âà0.73 ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏° single model.</li>
  <li><b>Decision Tree:</b> Recall ‚âà0.68 ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏π‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö interpretability.</li>
  <li><b>KNN:</b> Accuracy ‡∏™‡∏π‡∏á (~0.91) ‡πÅ‡∏ï‡πà Recall ‡∏ï‡πà‡∏≥ (~0.04).</li>
  <li><b>Voting / Stacking (meta-LR):</b> Recall ‚âà0.71 ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡πÅ‡∏•‡∏∞‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Å‡∏ß‡πà‡∏≤ SVM.</li>
  <li><b>Stacking (meta-RF):</b> ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡πÉ‡∏ô‡πÇ‡∏à‡∏ó‡∏¢‡πå imbalance ‚Äî Recall ‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å.</li>
</ul>

<h2>Trade-off &amp; Business Choice</h2>
<p>‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á:</p>
<ul>
  <li><b>‡∏ñ‡πâ‡∏≤‡πÄ‡∏ô‡πâ‡∏ô Recall ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î:</b> ‡πÉ‡∏ä‡πâ SVM-RBF (balanced) ‡∏´‡∏£‡∏∑‡∏≠ Stacking(meta=LogReg).</li>
  <li><b>‡∏ñ‡πâ‡∏≤‡πÄ‡∏ô‡πâ‡∏ô‡∏™‡∏°‡∏î‡∏∏‡∏•:</b> ‡πÉ‡∏ä‡πâ Logistic Regression ‡πÅ‡∏•‡πâ‡∏ß‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà F1 ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (Recall ~0.60‚Äì0.70).</li>
  <li><b>‡∏ñ‡πâ‡∏≤‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á:</b> ‡πÉ‡∏ä‡πâ Calibration + threshold ‡∏ï‡∏≤‡∏°‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (‡πÄ‡∏ä‡πà‡∏ô ‡∏ó‡∏µ‡∏° fraud investigation).</li>
</ul>

<h2>Lessons Learned</h2>
<ul>
  <li>‡πÇ‡∏à‡∏ó‡∏¢‡πå imbalance ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏ metric ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡πÉ‡∏ô Pipeline ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô leakage.</li>
  <li>SMOTE ‡∏Ñ‡∏ß‡∏£‡∏ó‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô train/CV ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‚Äî ‡πÉ‡∏ä‡πâ class_weight ‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏£‡πá‡∏ß.</li>
  <li>Threshold ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠ policy ‡∏à‡∏£‡∏¥‡∏á.</li>
  <li>Ensemble (Voting/Stacking) ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏ô‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ trade-off ‡πÑ‡∏î‡πâ‡∏î‡∏µ.</li>
</ul>

<h2>Final Deliverables</h2>
<ul>
  <li>Baseline (‡∏Ç‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô) + Rebuild Notebook (Confirm reproducibility).</li>
  <li>Experiments: Preprocessing / Hyperparameter tuning / Threshold &amp; Calibration / Ensemble.</li>
  <li>Summary table + Charts (Accuracy, Precision(1), Recall(1)).</li>
  <li>Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å threshold ‡∏ï‡∏≤‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ Recall ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡πà‡∏≠‡∏ô‚Äì‡∏´‡∏•‡∏±‡∏á.</li>
</ul>
"""

# ============================================
# üßæ Final Summary Table of All Models
# ============================================

import pandas as pd
from IPython.display import display, HTML
from tabulate import tabulate

# ‡∏£‡∏ß‡∏° Traditional ML + Deep/Boosted Models
df_summary = pd.DataFrame([
    # --- Baseline & Classical Models ---
    {"Model": "Baseline ‚Äì Logistic Regression", "Technique": "Default (thr=0.50)", "Accuracy": 0.566, "Precision(1)": 0.0857, "Recall(1)": 0.580},
    {"Model": "LogReg (tuned)", "Technique": "Balanced + Scaling", "Accuracy": 0.5638, "Precision(1)": 0.0884, "Recall(1)": 0.6052},
    {"Model": "LogReg (thr=0.40)", "Technique": "Lower threshold (Recall optimization)", "Accuracy": 0.2752, "Precision(1)": 0.0751, "Recall(1)": 0.8866},
    {"Model": "SVM (RBF, balanced)", "Technique": "Tuned (C=5, gamma=0.01)", "Accuracy": 0.4791, "Precision(1)": 0.0869, "Recall(1)": 0.7290},
    {"Model": "Decision Tree", "Technique": "Entropy, max_depth=5", "Accuracy": 0.5522, "Precision(1)": 0.0942, "Recall(1)": 0.6753},
    {"Model": "KNN (scaled)", "Technique": "k=3, Manhattan, distance", "Accuracy": 0.9106, "Precision(1)": 0.0920, "Recall(1)": 0.0407},


    # --- Advanced Models ---
    {"Model": "Random Forest (Bagging Ensemble)", "Technique": "Balanced + tuned", "Accuracy": 0.5561, "Precision(1)": 0.0957, "Recall(1)": 0.6814},
    {"Model": "LightGBM", "Technique": "Class_weight='balanced'", "Accuracy": 0.6384, "Precision(1)": 0.0974, "Recall(1)": 0.5446},
    {"Model": "XGBoost", "Technique": "scale_pos_weight", "Accuracy": 0.6624, "Precision(1)": 0.0989, "Recall(1)": 0.5100},
    {"Model": "MLP Neural Net", "Technique": "SMOTE + ReLU + Adam", "Accuracy": 0.6390, "Precision(1)": 0.0895, "Recall(1)": 0.4900},
        {"Model": "Voting Ensemble", "Technique": "LR + DT + SVM (soft voting)", "Accuracy": 0.5145, "Precision(1)": 0.0915, "Recall(1)": 0.7152},
    {"Model": "Stacking (meta=LogReg)", "Technique": "LR + DT + SVM ‚Üí meta-LR", "Accuracy": 0.5252, "Precision(1)": 0.0931, "Recall(1)": 0.7126},
    {"Model": "Stacking (meta=RF)", "Technique": "LR + DT + SVM ‚Üí meta-RF", "Accuracy": 0.9334, "Precision(1)": 0.2222, "Recall(1)": 0.0052},
])

# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì F1 Score ‡πÄ‡∏û‡∏¥‡πà‡∏°
df_summary["F1(1)"] = 2 * (df_summary["Precision(1)"] * df_summary["Recall(1)"]) / (
    df_summary["Precision(1)"] + df_summary["Recall(1)"]
)

# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏ô Colab (HTML version)
display(HTML(df_summary.to_html(index=False, float_format=lambda x: f"{x:.3f}")))

# ‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô console ‡πÅ‡∏ö‡∏ö‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏° (tabulate)
print("="*60)
print("Summary Table of Models")
print("="*60)
print(tabulate(df_summary, headers="keys", tablefmt="pretty", showindex=True, floatfmt=".4f"))

import matplotlib.pyplot as plt

plt.figure(figsize=(8,4))
plt.bar(df_summary["Model"], df_summary["Accuracy"], color="skyblue")
plt.title("Accuracy Comparison")
plt.ylabel("Accuracy")
plt.xticks(rotation=20, ha="right")
plt.tight_layout()
plt.show()

plt.figure(figsize=(10,5))
x = df_summary["Model"]
plt.bar(x, df_summary["Accuracy"], width=0.4, label="Accuracy", color="skyblue")
plt.bar(x, df_summary["Recall(1)"], width=0.4, label="Recall(1)", color="orange", alpha=0.7)
plt.title("Accuracy vs Recall(1) Comparison")
plt.ylabel("Score")
plt.xticks(rotation=20, ha="right")
plt.legend()
plt.tight_layout()
plt.show()

# ============================================
# üìà Visualization: Model Performance Comparison
# ============================================

import matplotlib.pyplot as plt
import numpy as np

# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
models = df_summary["Model"]
accuracy = df_summary["Accuracy"]
recall = df_summary["Recall(1)"]
precision = df_summary["Precision(1)"]

x = np.arange(len(models))
width = 0.25  # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ó‡πà‡∏á

plt.figure(figsize=(10,5))
plt.bar(x - width, accuracy, width, label="Accuracy", color="#9ad1f7")
plt.bar(x, recall, width, label="Recall (class 1)", color="#f7b267")
plt.bar(x + width, precision, width, label="Precision (class 1)", color="#8fd694")

plt.title("Model Performance Comparison", fontsize=14, weight="bold")
plt.ylabel("Score")
plt.xticks(x, models, rotation=25, ha="right")
plt.legend()
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.tight_layout()
plt.show()

from sklearn.metrics import roc_curve, auc

# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° y_score ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ
plt.figure(figsize=(10,8))
plt.plot([0,1],[0,1],'k--', label='Random Guess (AUC=0.50)')

for name, y_score in pr_candidates:
    fpr, tpr, _ = roc_curve(y_test, y_score)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{name} (AUC={roc_auc:.3f})")

plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (TPR)")
plt.title("ROC Curve Comparison")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

import pandas as pd

# ==== ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤ ====
df_old = pd.DataFrame({
    "Model": [
        "Logistic Regression (Standard)",
        "Logistic Regression (Polynomial)",
        "SVM (Linear)",
        "SVM (Polynomial)",
        "SVM (RBF)",
        "Decision Tree",
        "Random Forest",
        "KNN",
        "Naive Bayes"
    ],
    "Accuracy": [
        0.5658778017977016,
        0.5398793946979179,
        0.5431220844237115,
        0.4894754807145295,
        0.4944248492433724,
        0.8451473432699966,
        0.8637501422232335,
        0.7044032313118671,
        0.26282853566958697
    ]
})

# ==== ‡∏Ç‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà ====
df_new = pd.DataFrame({
    "Model": [
        "Logistic Regression (balanced + scaling)",
        "SVM-RBF (balanced + scaling)",
        "KNN (scaled)",
        "Decision Tree (balanced)",
        "Logistic Regression (+SMOTE + scaling)",
        "SVM-RBF (+SMOTE + scaling)",
        "KNN (+SMOTE + scaling)",
        "Decision Tree (+SMOTE)"
    ],
    "Accuracy": [
        0.5638,
        0.4791,
        0.9106,
        0.5522,
        0.5660,
        0.5419,
        0.6575,
        0.3618
    ],
    "Precision(1)": [
        0.0884,
        0.0869,
        0.0920,
        0.0942,
        0.0858,
        0.0875,
        0.0788,
        0.0756
    ],
    "Recall(1)": [
        0.6052,
        0.7290,
        0.0407,
        0.6753,
        0.5801,
        0.6329,
        0.3939,
        0.7766
    ],
    "F1(1)": [
        0.1542,
        0.1553,
        0.0564,
        0.1654,
        0.1494,
        0.1537,
        0.1313,
        0.1379
    ]
})

import matplotlib.pyplot as plt
import numpy as np

# ‡∏£‡∏ß‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
models_all = list(df_old["Model"]) + list(df_new["Model"])
acc_all = list(df_old["Accuracy"]) + list(df_new["Accuracy"])
labels_oldnew = (["Old"] * len(df_old)) + (["New"] * len(df_new))

df_all = pd.DataFrame({
    "Model": models_all,
    "Accuracy": acc_all,
    "Group": labels_oldnew
})

plt.figure(figsize=(11,5))
x = np.arange(len(df_all))
colors = ["skyblue" if g == "Old" else "orange" for g in df_all["Group"]]

plt.bar(df_all["Model"], df_all["Accuracy"], color=colors)
plt.xticks(rotation=25, ha="right")
plt.ylabel("Accuracy")
plt.title("Accuracy Comparison: Old vs New Models")
plt.tight_layout()
plt.show()